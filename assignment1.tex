\documentclass{article}
\usepackage{mathrsfs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{braket}
\usepackage[normalem]{ulem} % for strikeout line
% \usepackage{graphicx}
% \usepackage{epstopdf}

%-------------------------------------------------------%
\newcounter{pcounter}                                   %
\newenvironment{problem}                                %
{                                                       %
    \stepcounter{pcounter}                              %
    \textbf{\arabic{pcounter}.}                         %
}{}                                                     %
\newenvironment{solution}                               %
{\textbf{Solution:} \\}{$\blacksquare$\newline}         %
%-------------------------------------------------------%
\newcommand{\tab}{\ \ \ \ }                             %
\newcommand{\leadto}{\Rightarrow}                       %
\newcommand{\domR}{\mathcal{R}}                         %
\newcommand{\domS}{\mathbb{S}}                          %
\newcommand{\Gaussian}{\mathcal{N}}                     %
\newcommand{\IdenMat}{\textit{I}}                       %
\newcommand{\abss}[1]{\| #1 \|}                         %
\newcommand{\tr}[1]{\textbf{tr}(#1)}                    %
\newcommand{\vecOne}{\textbf{1}}                        %
%-------------------------------------------------------%

\begin{document}
    %------------------- The Title -------------------%
    \parindent 0in
    \parskip 1em
    \title{COMP8802 Assignment 1 Solution Sheet}
    \author{3030058647, HONG Yuncong}
    \maketitle

    %=================== Problem 1 ===================%
    \begin{problem}
        We have two random variables $X$ and $Y$.
        X follows the standard Gaussian distribution:
        $$
        p(x) = \Gaussian(x | 0, \IdenMat),
        $$
        where $x$ is a $m$-dim vector, and $\IdenMat$ is the identity matrix. $Y|X$ also follows a Gaussian distribution:
        $$
        p(y|x) = \Gaussian(y | \mu+\Lambda x, \Psi),
        $$
        where $y$ is a $n$-dim vector, $\Lambda \in \domR^{n \times m}$ is called the factor loading matrix, $\mu$ is a constant vector, and $\Psi$ is a \textbf{diagonal covariance matrix}.

        (1) [Joint Distribution] Prove that the joint distribution of $(X, Y)$ is
        $$
        p(
            \begin{pmatrix}
                x \\ y
            \end{pmatrix}
        ) = 
        \Gaussian(
            \begin{pmatrix}
                x \\ y
            \end{pmatrix}
            |
            \begin{pmatrix}
                0 \\ \mu
            \end{pmatrix}
            ,
            \begin{pmatrix}
                \IdenMat & \Lambda^T \\
                \Lambda & \Lambda \Lambda^T + \Psi
            \end{pmatrix}
        )
        $$

        (2) [Posterior Distribution] Prove that the posterior distribution $X|Y$ is $p(x|y) = \Gaussian(x|\mu_{x|y}, \Sigma_{x|y})$, where
        \begin{gather*}
            \mu_{x|y} = \Sigma \Lambda^T \Psi^{-1} (Y - \mu) \\
            \Sigma_{x|y} = (\IdenMat + \Lambda^T \Psi^{-1} \Lambda)^{-1}
        \end{gather*}

        (3) [Incomplete log likelihood] The incomplete log likelihood is defined according to the marginal density of $Y$:
        $$
        l(\theta, \mathcal{D}) = log \prod\limits_{i=1}^{n} p(y_i)
        $$
        \tab a) Please compute the incomplete log likelihood, which should involve $\Lambda, \Psi$, and adata covariance matrix 
        $S=\sum_{i=1}^n (y_i - \mu)(y_i - \mu)^T$ \\
        \tab b) Try to compute the MLE estimate for $\theta = (\Lambda, \Psi)$ and explain what is the main difficulty that prevents you from getting the result.

        (4) [Complete log likelihood] The complete log likelihood is defined as 
        $$
        l_c(\theta, \mathcal{D}) = log \prod\limits_{i=1}^{n} p(x_i, y_i)
        $$
        \tab a) Please compute the complete log likelihood, which should involve $\Lambda, \Psi$, and adata covariance matrix 
        $S=\frac{1}{n} \sum_{i=1}^n (y_i - \Lambda x_i)(y_i - \Lambda x_i)^T$ \\
        \tab b) Try to compute the MLE estimate for $\theta = (\Lambda, \Psi)$ and explain why it is simpler that 1.3(b).
    \end{problem}

    \begin{solution}
        (1) $p(x, y) = \sum_x p(y|x) p(x)$, while
        $$
        p(x) = \Gaussian(x | 0, \IdenMat)
        \\
        p(y|x) = \Gaussian(y | \mu+\Lambda x, \Psi)
        \\
        $$
    \end{solution}

    %=================== Problem 2 ===================%
    \begin{problem}
        Let $X$ be an i.i.d collection of random variables from a Poisson distribution with parameter $\lambda$, $X \sim Poisson(\lambda)$, where $\lambda > 0$

        (1) [Basic Concepts and Properties]

        \tab a) Write out the exponential family form of X
        
        \tab b) Determine the sufficient statistics of $X$ for the Poisson distribution, i.e. $T(x)$
        
        \tab c) Write out the log-partition function of $X$, i.e. $A(\eta)$
        
        \tab d) Determine the response function (i.e., the inverse of the link function)
        
        \tab e) Determine $E[X]$ and $Var[X]$ using the log-partition function
        
        \tab f) Write out the MLE of $\lambda$ for data $X=\{x_1, \dots, x_n\}$
        
        \tab g) Let $\lambda \sim Gamma(\alpha, \beta)$, where $\alpha, \beta > 0$. The Gamma distribution is conjugate to the Poisson distribution,$ Gamma(\alpha, \beta) \propto x^{\alpha - 1} / e^{\beta x}$. Write out the MAP of $\lambda$ given $X=\{x_1, \dots, x_n\}$ .

        (2) [MLE and MAP simulation] \\
        For this problem, \textbf{please use the data labled HW1.txt}. This file contains a column array of 10,000 integer values, where each value represents the number of customers that entered a 24-hour laundromat in one hour intervals, over 10,000 hours.
        Let $X = \{x_1, \dots, x_t, \dots, x_n\}$ represents this column array, where $x_t$ is the number of customers that entered during the $t$-th hour. The hourly arrival of customers can be modeled as a collection of i.i.d. random variables drawn from a Poisson distribution $X \sim Poisson(\lambda)$, where $\lambda$ is the hourly arrival rate.
        
        \tab a) Plot a histogram of $X$ using 25 bins.
        
        \tab b) Using your answer from 2.1, compute the MLE of $\lambda$ for the observed data $X$ 

        For parts c) - e) model the hourly arrival rate $\lambda$ as having a Gamma distribution $\lambda \sim Gamma(\alpha, \beta)$, where $\alpha, \beta > 0$. Use your answer from 2.1(g) to:
        
        \tab c) Compute the MAP of $\lambda$ given $X$ for $\alpha = 1$ and $\beta = 1$
        
        \tab d) Compute the MAP of $\lambda$ given $X$ for $\alpha = 100$ and $\beta = 1$
        
        \tab e) Compute the MAP of $\lambda$ given $X$ for $\alpha = 10$ and $\beta = 1$

        \tab f) which approximation to $\lambda$ in b)-e) do you think is the best? How much does the prior distribution, and parameterizations of the prior in particular, impact the MAP estimates of $\lambda$? (One or Two sentences)
    \end{problem}

    \begin{solution}
        
    \end{solution}
\end{document}