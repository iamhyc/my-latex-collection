\documentclass{article}
\usepackage{mathrsfs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{braket}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{float}
\usepackage[noend]{algorithm2e}
\usepackage[normalem]{ulem} % for strikeout line
\usepackage{graphicx}
\graphicspath{ {./images/} }
% \usepackage{epstopdf}

%-------------------------------------------------------%
\newcounter{pcounter}                                   %
\newenvironment{problem}                                %
{                                                       %
    \color{gray}                                        %
    \stepcounter{pcounter}                              %
    \textbf{\arabic{pcounter}.}                         %
}{}                                                     %
\newenvironment{solution}                               %
{\textbf{Solution:} }{$\blacksquare$}                   %
%-------------------------------------------------------%
\newcommand{\tab}{\ \ \ \ }                             %
\newcommand{\leadto}{\Rightarrow}                       %
\newcommand{\domR}{\mathcal{R}}                         %
\newcommand{\domS}{\mathbb{S}}                          %
\newcommand{\Gaussian}{\mathcal{N}}                     %
\newcommand{\IdenMat}{\textit{I}}                       %
\newcommand{\abss}[1]{\| #1 \|}                         %
\newcommand{\tr}[1]{\textbf{tr}(#1)}                    %
\newcommand{\vecOne}{\textbf{1}}                        %
\renewcommand{\vec}[1]{\mathbf{#1}}                     %
%-------------------------------------------------------%

\begin{document}
    %------------------- The Title -------------------%
    \parindent 0in
    \parskip 1em
    \title{COMP9501 Assignment 4 Solution Sheet}
    \author{HONG Yuncong, 3030058647}
    \maketitle

    \begin{section}{Problem 1}
        \setcounter{pcounter}{0}
        We have discussed the \emph{Baum-Welch algorithm} in class, which is used to find the unknown parameters $(a,b,\pi)$ of a hidden Markov model (HMM),
        where $a$ is the stochastic transition matrix, $b$ is the matrix for the emission probability, and $\pi$ is the initial state distribution. The lecture provides the backbone of the Baum-Welch algorithm, now we are going to write down everything in detail.

        The Baum-Welch is an iterative algorithm, which starts with random initial conditions for $(a,b,\pi)$. They can also be set using prior information about the parameters if available; this can speed up the algorithm and also steer it toward the desired local optimum. Each iteration will have two parts: 1) the forward/backward stage; 2) the update stage.

        %=================== Problem 1.1 ===================%
        \begin{problem}
            [Forward/Backward Stage]

            \begin{enumerate}[label=\alph*)]
                \item Using the \textit{message passing framework} that we discussed in class to show that the backward/backward can be written using the parameter $(a,b,\pi)$ computed in the last iteration, where the forward process is:
                $$
                \alpha_t(i) = \sum_j a_{t-1}(j) \cdot a_{j,i} \cdot b_{j,x_{t-1}}
                $$
                and the backward process is:
                $$
                \beta_t(i) = \sum_j \beta_{t+1}(j) \cdot b_{j,x_{t+1}} \cdot a_{i,j}
                $$
                and $\alpha_1 = \pi, \beta_T = \vecOne$

                \item Derive the probabilistic meaning for $\alpha$ and $\beta$:
                $$
                \alpha_t(i) = p(x_1, \dots, x_{t-1}, y_t=i)
                $$
                and
                $$
                \beta_t(i) = p(x_{t+1}, \dots, x_T | y_t=i)
                $$
            \end{enumerate}
        \end{problem}

        \begin{solution}
            \begin{enumerate}[label=\alph*)]
                \item % Answer for (a)
                Given that the definition of forward message $\alpha_k$ in HMM for the observed state $y_k$ is described as follow:
                $$
                \alpha_k(y_k) = \sum_{y_{k-1}} \alpha_{k-1}(y_{k-1}) \cdot p(y_k|y_{k-1})
                $$
                Assume that at $k$-th stage, we have $y_k=i$; and furthermore the expression for $p(y_k | y_{k-1})$ could be described with transition matrix $a$ and emission matrix $b$ as:
                $$
                p(y_k | y_{k-1}, x_{k-1}) = a_{y_{k-1}, y_k} \cdot b_{j, x_{k-1}}
                $$
                And bind up the two formulas we come up with:
                $$
                \alpha_t(i) = \sum_j \alpha_{t-1}(j) \cdot a_{j,i}b_{j,x_{t-1}}
                $$
                And similarly for backward message $\beta_k$ in HMM we have:
                $$
                \beta_t(i) = \sum_j \beta_{t+1}(j) \cdot b_{j,x_{t+1}}a_{i,j}
                $$

                \item % Answer for (b)
                Given that $\alpha_1=\pi$, then for the forward elimination process, we have:
                \begin{align*}
                    \alpha_2(i) &= \sum_j \pi_j \cdot b_{j,x_1}a_{j,i} = p(x_1,y_2=i)
                    \\
                    \alpha_3(i) &= \sum_j \pi_j \cdot b_{j,x_2}a_{j,i} = p(x_1,x_2,y_3=i)
                    \\
                    & \dots\ \dots
                \end{align*}
                Then we come up with:
                $$
                \alpha_t(i) = p(x_1, \dots, x_{t-1}, y_t=i)
                $$
                which implies the the forward message $\alpha_t$ is the probability distribution of series of observing states from start to time $t-1$. \\
                Similarly, given $\beta_T=\vecOne$, for backward elimination process, we have:
                \begin{align*}
                    \beta_{T-1} &= \sum_j \vecOne \cdot b_{j,x_T}a_{i,j} = p(x_T|y_{T-1}=i)
                    \\
                    \dots \dots
                    \\
                    \beta_t(i) &= p(x_{t+1}, \dots, x_T | y_t=i)
                \end{align*}
                which implies that the backward message $\beta_t$ is the probability distribution of series of observing states from time $t-1$ to ending.
                
                And the heuristic conclusion is that: $p(i)=\alpha_t(i) \cdot \beta_t(i)$.
            \end{enumerate}
        \end{solution}

        %=================== Problem 1.2 ===================%
        \begin{problem}
            [Update] \\
            In the update stage, we actually are performing the EM algorithm to optimize the expected log-likelihood for HMM.
            \begin{enumerate}[label=\alph*)]
                \item In the E-step, we need to compute $\gamma_{t,i}=p(y_t=i|x_1, \dots, x_T)$ and $\xi_{t,i,j} = p(y_{t-1}=i, y_{t}=j|x_1, \dots, x_T)$.
                Write down the details about how to compute $\gamma$ and $\xi$ using the $\alpha$ nad $\beta$ that we computed.

                \item after E-step, we fill in the values of unknown hidden variables using their sufficient statistics. Then, we can solve the "completed" version of the log-likelihood and compute the updated parameters. Follow the lecture, derive the update by yourself.
            \end{enumerate}
        \end{problem}

        \begin{solution}
            \begin{enumerate}[label=\alph*)]
                \item % Answer for (a)
                With $\alpha, \beta$ given, $\gamma$ could be obtained by:
                \begin{align*}
                    \gamma_{t,i} &= p(y_t=i | x_1, \dots, x_T) \\
                    &= \frac{p(x_1, \dots, x_T, y_t=i)}{p(x_1, \dots, x_T)} \\
                    &= \frac{\alpha_t(i)\beta_t(i) \cdot p(x_t|y_t=i)}{\sum_j \alpha_t(j)\beta_t(j) \cdot p(x_t|y_t=j)} \\
                    &= \frac{\alpha_t(i)\beta_t(i) \cdot b_{i,x_t}}
                            {\sum_j \alpha_t(j)\beta_t(j) \cdot b_{j,x_t}}
                \end{align*}
                and $\xi$ could be obtained by:
                \begin{align*}
                    \xi_{t,i,j} &= p(y_{t-1}=i,y_t=j | x_1,\dots,x_T) \\
                    &= \frac{p(x_1,\dots,x_T, y_{t-1}=i, y_{t}=j)}{p(x_1, \dots, x_T)} \\
                    &= \frac{a_{i,j} \cdot (\alpha_{t-1}(i)\beta_{t}(j)) \cdot (b_{i,x_{t-1}}b_{j,x_t})}
                            {\sum_j \alpha_t(j)\beta_t(j) \cdot b_{j,x_t}}
                \end{align*}
                
                \item % Answer for (b)
                According to the \textit{Lecture 9} slides, we have expected completed log-likelihood in the following expression:
                \begin{align*}
                    \langle{l_c(a,b,\pi)}\rangle &=
                    \sum_{n=1}^N \langle{y_1^{(n)}}\rangle^T \log{\pi} \\
                    & + \sum_{n=1}^{N}\sum_{t=2}^{T} tr(\langle{ y_{t-1}^{(n)}( y_t^{(n)} )^T}\rangle \log(a)) \\
                    & + \sum_{n=1}^{N}\sum_{t=1}^{T} tr( x^{(n)}_t \langle{y^{(n)}_t}\rangle^T \log{b})
                \end{align*}
                where: $\langle{y^{(n)}_{1,i}}\rangle = \gamma^{(n)}_{1,i}, 
                \langle{y^{(n)}_{t,i}}\rangle = \gamma^{(n)}_{t,i};
                \langle{y^{(n)}_{t-1,i}(y^{(n)}_t)}\rangle = \xi^{(n)}_{t,i,j}$.\\
                With setting the derivative w.r.t 3 parameters to zero, we could have the MLE for the three parameters as:
                \begin{align*}
                    \pi_{i}^{MLE} &= \frac{\sum_n \gamma^{(n)}_{t,i}}{N}\\
                    a_{i,j}^{MLE} &= \frac{\sum_n\sum_{t=2}^T \xi^{(n)}_{t,i,j}}{\sum_n\sum_{t=1}^{T-1} \gamma^{(n)}_{t,i}}\\
                    b_{i,k}^{MLE} &=
                                \frac{\sum_n\sum_{t=1}^T \gamma^{(n)}_{t,i} x^{(n)}_{t,k}}
                                {\sum_n\sum_{t=1}^T \gamma^{(n)}_{t,i}}
                \end{align*}

            \end{enumerate}
        \end{solution}
        
        %=================== Problem 1.3 ===================%
        \begin{problem}
            [Choose different $\alpha$ and $\beta$]\\
            If you check the standard literature for HMM, you will find that the forward/backward is slightly different. In particular, one alternative way to define the messages are:
            \begin{gather*}
                \alpha_t(y_t) = p(x_1, \dots, x_t, y_t)
                \\
                \beta_t(y_t)  = p(x_{t+1}, \dots, x_T|y_t)
            \end{gather*}
            Please derive the entire HMM, including the forward/backward and the parameter update using such slightly different messages.
        \end{problem}

        \begin{solution}
            Given the definition for forward and backward message, $\alpha_t(i)$ and $\beta_t(i)$ meets at $y_t$.\\
            For the forward elimination process:
            \begin{align*}
                \alpha_t(i) &= p(x_1, \dots, x_t, y_t=i)\\
                &= \sum_j p(x_1, \dots, x_{t-1}, y_{t-1}=j) \cdot a_{j,i} \cdot p(x_t|y_t=i) \\
                &= \sum_j \alpha_{t-1}(j) \cdot a_{j,i}b_{i,x_t}
            \end{align*}
            For the backward elimination process:
            \begin{align*}
                \beta_t(i) &= p(x_{t+1}, \dots, x_T| y_t=i) \\
                &= \sum_j p(x_{t+2}, \dots, x_T|y_{t+1}=j) \cdot a_{j,i} \cdot p(x_{t+1}|y_{t+1}=j) \\
                &= \sum_j \beta_{t+1}(j) \cdot a_{i,j}b_{j,x_{t+1}}
            \end{align*}

            Then for EM-algorithm, we firstly compute $\gamma_{t,i}$ and $\xi_{t,i,j}$ in E-step:
            \begin{align*}
                \gamma_{t,i} &= p(y_t=i | x_1, \dots, x_T) \\
                &= \frac{p(x_1, \dots, x_t, y_t=i)\cdot p(x_{t+1},\dots,x_T|y_t=i)}{p(x_1, \dots, x_T)} \\
                &= \frac{\alpha_t(i)\beta_t(i)}{\sum_j \alpha_t(j)\beta_t(j)}
                \\
                \xi_{t,i,j} &= p(y_{t-1}=i, y_t=j|x_1, \dots, x_T) \\
                &= \frac{\alpha_{t-1}(i)\beta_t(j)\cdot a_{i,j}b_{j,x_t}}
                    {\sum_j \alpha_t(j)\beta_t(j)}
            \end{align*}
            In M-step, the log-likelihood is of the same expression as the expression for $\gamma, \xi$ is re-formulated above. So, the parameter update is same as:
            \begin{align*}
                \pi_{i}^{MLE} &= \frac{\sum_n \gamma^{(n)}_{t,i}}{N}\\
                    a_{i,j}^{MLE} &= \frac{\sum_n\sum_{t=2}^T \xi^{(n)}_{t,i,j}}{\sum_n\sum_{t=1}^{T-1} \gamma^{(n)}_{t,i}}\\
                    b_{i,k}^{MLE} &=
                                \frac{\sum_n\sum_{t=1}^T \gamma^{(n)}_{t,i} x^{(n)}_{t,k}}
                                {\sum_n\sum_{t=1}^T \gamma^{(n)}_{t,i}}
            \end{align*}
        \end{solution}
    \end{section}

    \begin{section}{Problem 2}
        \setcounter{pcounter}{0}
        Let's go back and look at the Problem 1 in Assignment 1. As we discussed here, if both $X$ and $Y$ are observable, then we can maximize the complete log-likelihood to easily compute the MLE of the parameters. However, in practice, we only observe $Y$ and this results in an incomplete log-likelihood and thus the optimizations is difficult.

        Similar to HMM, we can also use the EM algorithm to solve the MLE of the incomplete log-likelihood recursively.

        %=================== Problem 2.1 ===================%
        \begin{problem}
            In the E-step, we need to write down the expected complete log-likelihood, i.e. $\langle{l_c(\theta,D)}\rangle = E_{X|y;\theta^{(t-1)}[l_c(\theta,D)]}$. Based on what you get from Assignment 1, write down the result.
        \end{problem}

        \begin{solution}
            The complete log-likelihood we obtained in \textit{Assignment 1} is:
            \begin{align*}
                l(\theta, \mathcal{D}) &= l\sum_i \log{p(x_i, y_i)} \\
                &= -\frac{1}{2} \sum_{i=1}^n tr(x_i x_i^T)
                   - \frac{N}{2} \log{|\Psi|} - \frac{N}{2} tr(\Psi^{-1}S)
            \end{align*}
            where the covariance matrix $S=\frac{1}{N}\sum_{i=1}^N (y_i-\Lambda x_i)(y_i-\Lambda x_i)^T$.\\
            The we have expected log-likelihood as:
            $$
            \langle{l_c(\theta, \mathcal{D})}\rangle = 
                -\frac{1}{2} \sum_{i=1}^n tr(\langle{x_i x_i^T}\rangle)
                - \frac{N}{2} \log{|\Psi|} - \frac{N}{2} tr(\Psi^{-1}\langle{S}\rangle)
            $$
            So, for E-step we have the following expectations:
            \begin{gather*}
                \langle{S}\rangle = \frac{1}{N} \sum_{n=1}^{N} (y_n y_n^T - y_n \langle{X_N}\rangle^T \Lambda^T - \Lambda \langle{X_n}\rangle Y_n^T + \Lambda \langle{X_n X_N^T}\rangle \Lambda^T)\\
                \langle{X_n}\rangle = E[X_n|y_n] = \mu_{x|y} \\
                \langle{X_n X_N^T}\rangle = Var[X_n|y_n] + E[X_n|y_n] E[X_n|y_n]^T = \Sigma_{x|y} + \mu_{x|y} \mu_{x|y}^T\\
            \end{gather*}
            where:
            \begin{gather*}
                \mu_{x|y} = \Sigma_{x|y} \Lambda^T \Psi^{-1} (y-\mu) \\
                \Sigma_{x|y} = (I + \Lambda^T \Psi^{-1} \Lambda)^{-1}
            \end{gather*}
        \end{solution}

        %=================== Problem 2.2 ===================%
        \begin{problem}
            In the M-step, we need to update the parameters. Please write down the details.
        \end{problem}

        \begin{solution}
            For M-step, by taking partial derivatives w.r.t $\Psi, \Lambda$ respectively of the complete log-likelihood function, we obtain the two update rules:
            \begin{gather*}
                \Psi^{t+1} = \langle{S}\rangle
                \\
                \Lambda^{t+1} = (\sum_n y_n \langle{X_n^T}\rangle) (\sum_n  \langle{X_n X_n^T}\rangle)^{-1}
            \end{gather*}
        \end{solution}
    \end{section}

\end{document}