\section{Problem Formulation}
%----------------------------------------------------------------------------------------%
In this section, we formulate the optimization problem under standard MDP framework, which aims at optimizing online dispatching decisions of all APs.
Given that each AP makes decisions based on collected global state information, we describe the system state and policy from a global perspective, where each AP adopt its dispatching policy mapping from the collected system information.
Due to the different distributions of \brdelay~for each AP, APs use the stale policy in one interval firstly, and then update their policy at different time slots in an unknown order.
\fixit{
    At the end of this section, we show that the solution suffers from curse of dimensionality and a low-complexity solution is needed.
}

\subsection{System State and Dispatching Policy}
The system state is selected from the global perspective based on the broadcast information.
We consider all the APs maintain the same global information, which is collected and updated by broadcast at the beginning of each interval.
As each AP would update its dispatching policy based on the latest broadcast information, the \brdelay{s} in each interval is also taken as a part of state information.
The \brdelay~relative to the broadcast points and time points for policy update is depicted in Fig. \ref{fig:brd-trans}.
\begin{definition}[System State]
    The system state at $i$-th \brpoint~is denoted as follows.
    \begin{align}
        \Stat(t_i) \define \Paren{
            \Obsv(t_{i-1}), \Obsv({t_{i}}), \mathcal{D}(t_{i})
        },
    \end{align}
    where $\mathcal{D}(t_{i}) \define \set{D_{1}(t_{i}), \dots, D_{K}(t_{i})}$ denotes the \brdelay for each AP, $\Obsv(t_{i})$ denotes updated information after receiving the whole broadcasting information, and $\Obsv(t_{i-1})$ denotes the stale information which impact on the policy adopted in current interval ($i=1,2,\dots$).
\end{definition}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.80\textwidth]{brd-trans.pdf}
    \caption{Global System Transition with Partial Information-based Dispatching Decision}
    \label{fig:brd-trans}
\end{figure}


\fixit{
    Given the fact that AP nodes would only update their own global information only after \brdelay in each broadcast interval, the state and policy in formulated MDP problem are actually composed of both stale and updated information due to \brdelay.
    Based on the two stages of global broadcast information illustrated in system state, the compounded global-wise policy of all AP nodes is defined as follows.
}
\begin{definition}[Compounded Dispatching Policy]
    The compounded dispatching policy $\Policy(\Stat(t_{i}))$ over $\Stat(t_{i})$ which is defined with two-stage broadcasting information which is given as follows.
    \begin{align}
        \Policy\Paren{\Stat(t_{i})} \define \Bracket{
            \tilde{\Policy}(\Obsv(t_{i})), \tilde{\Policy}(\Obsv(t_{i-1}))
        },
    \end{align}
    where
    $\tilde{\Policy}(\Obsv(t_{i})) \define [\tilde{\Omega}_{1}(\Obsv(t_{i})), \dots, \tilde{\Omega}'_{K}(\Obsv(t_{i}))]$.
    The policy mapping over system state is further decoupled onto the two-stage broadcast information.

    Furthermore,
    $\tilde{\Omega}_{k}(\Obsv(t_{i}))$ in $\tilde{\Policy}(\Obsv(t_{i}))$
    denotes the local policy mapping over $\Obsv(t_{i})$ for $k$-th AP ($\forall k\in\apSet$), i.e. $k$-th AP would change its policy from $\tilde{\Omega}_{k}(\Obsv(t_{i-1}))$ to $\tilde{\Omega}_{k}(\Obsv(t_{i}))$ when receiving $\Obsv(t_{i})$.
    The explicit definition is given as follows.
    \begin{align}
        &\tilde{\Omega}_{k}\Paren{\Obsv(t_{i})} \define \set{\omega_{k,j}(t_{i})|\forall m\in\esSet, j\in\jSpace}
        ~(\forall k\in\apSet),
        \label{def_action}
    \end{align}
    where $\omega_{k,j}(t_{i})$ denotes the deterministic dispatching policy that: when $k$-th AP with information $\Obsv(t_{i})$, it would then upload $j$-type job to $m$-th edge server if and only if $\omega_{k,j}(t_{i})=m$.
\end{definition}

We note that after the dispatching policy, the arrival distribution $A_{k,j}$ is further split onto different Edge Servers. The split arrival processes for $k$-th AP ($\forall k\in\apSet$) are still i.i.d Bernoulli distribution as
$\tilde{\lambda}^{(k)}_{m,j}(t_{i}) \define \lambda_{k,j} I[\omega_{k,j}(t_i) = m]$ ($\forall m\in\esSet, j\in\jSpace$), where $I[\cdot]$ is indicator function.
%----------------------------------------------------------------------------------------%

\subsection{The Optimization Problem}
We propose the jobs dispatching optimization problem with the target to minimize \emph{average response time} of all offloaded jobs in MEC system.
The \emph{average response time} is composed of uploading time from APs to edge servers, and queueing-and-service time on corresponding edge server. According to \emph{Little's Law}, the average response time of all jobs is equally as average number of jobs in system.
        
\fixit{
    Due to the intrinsic property of periodic information broadcasting, we collect the cost for number counting at the pace of broadcasting interval which could be seems as sampling of original process based on timeslot scale.
}
Besides the cost counted for job response time, we furthermore add penalty on jobs submission over its capacity limit when the over-limited jobs will be rejected at the end of current broadcast interval.
Therefore, the cost function of this problem is given as follows.
\begin{align}
    g\Paren{\Stat(t_{i}), \Policy(\Stat(t_{i}))} \define&
    \sum_{k\in\apSet}\sum_{m\in\esSet}\sum_{j\in\jSpace} \Inorm{\vec{r}^{(k)}_{m,j}(t_{i})} + \sum_{m\in\esSet}\sum_{j\in\jSpace}
    \nonumber\\
    &\Brace{
        l_{m,j}(t_{i}) + \beta \mat{I}[l_{m,j}(t_{i})=L_Q]
    },
\end{align}
where $\Inorm{\vec{r}^{(k)}_{m,j}(t_i)}$ denotes the $L^1$-norm of vector $\vec{r}^{(k)}_{m}$, i.e. the sum up of absolute value of each entry; $\beta$ is weight factor for full-queue penalty.
        
Our distributed optimization problem definition is given as follows.
\begin{problem}[Original Cooperative Job Dispatching Problem]
    \begin{gather}
        \min_{\Policy} \lim_{T \to \infty}
            \mathbb{E}_{\Policy}
                \Bracket{\sum_{i=1}^{T} \gamma^{i-1} g\Paren{\Stat(t_{i}), \Policy(\Stat(t_{i}))}|\Stat(t_1)},
    \end{gather}
    where the cost is collected with a discount factor $\gamma$.
    \delete{and $\Policy$ is optimized policy globally always with full-state information available.}
\end{problem}
According to \cite{sutton1998introduction}, the above problem could be solved by the following \emph{Bellman's equation}:
\begin{align}
    V(\Stat(t_i)) = &g(\Stat(t_i)) + \gamma \min_{\Policy(\Stat(t_{i}))}
        \nonumber\\
        & \sum_{\Stat(t_{i+1})} \Pr\{ \Stat(t_{i+1})|\Stat(t_{i}), \Policy(\Stat(t_{i})) \} \cdot V(\Stat(t_{i+1})).
    \label{sp_0}
\end{align}

However, the state information is actually partial-presence as $\Obsv(t_{i-1})$ at $t_i$ while $\Obsv(t_{i})$ would be available only after the random \emph{Update Latency} for each AP nodes.
This problem structure results into separated different acknowledge on when the policy should be updated for AP nodes. Thus we decouple the original problem into $K$-sub \hl{MDP-like problems} that update $\Policy$ in an iterative way which could at least converges to a sub-optimal solution.
        
For example, the $k$-th AP would update its policy in $k$-th sub-problem ($\forall k\in\apSet$) while the other AP nodes would keep with the previous policy. The corresponding Bellman's Equation is given as follows.
\begin{align}
    &V(\Stat(t_{i})) = g(\Stat_{i}) 
    \nonumber\\
    &~~~~+ \gamma \min_{\tilde{\Omega}_{k}(\Obsv(t_{i}))} \Pr\{ \Obsv(t_{i+1})|\Obsv(t_{i}), \Policy(\Stat(t_{i})) \} \cdot V(\Stat(t_{i+1})),
    \label{sp_k}
\end{align}
where $\tilde{\Omega}_{k}(\Obsv(t_{i}))$ is the component of $\Policy(\Obsv(t_{i}))$.
The performance improvement of next policy update is guaranteed by solution of Bellman's Equation with fixed policies from previous update.
The details and proof will be given in the following algorithm section.

To better analyze the optimization structure of the sub-problem, we decouple the transition function. The expression of transition function in Eqn. \ref{sp_k} is given as follows.
\begin{lemma}[Transition Function Decoupling]
    The transition function in Bellman's equation could be decoupled on states of APs and edge servers, which will facilitate the approximated value function expression in the following section.
    The decoupled transition function for Eqn. \ref{sp_k} is given as follows.
    \begin{align}
        & \Pr\{\Obsv(t_{i+1})|\Obsv(t_{i}), \Policy(\Stat(t_{i}))\}
        \nonumber\\
        =& \prod_{k\in\apSet}\prod_{m\in\esSet}\prod_{j\in\jSpace}
                \Pr\Brace{
                    \vec{r}^{(k)}_{m,j}(t_{i+1})|\vec{r}^{(k)}_{m,j}(t_{i}), \Policy(\Stat(t_{i}))
                }
                \times  
            \nonumber\\
            & \prod_{m\in\esSet}\prod_{j\in\jSpace}
                \Pr\Brace{
                    Q_{m,j}(t_{i+1})|Q_{m,j}(t_{i}), \mathcal{R}(t_{i}), \Policy(\Stat(t_{i}))
                }.
    \end{align}
\end{lemma}
\begin{proof}
    Proof deleted.
\end{proof}

The first part denotes the state transition on AP.
Denote as $\Pr\Brace{\vec{r}^{(k)}_{m,j}(t_{i+1})|\vec{r}^{(k)}_{m,j}(t_{i})} \sim \vecG{\Theta}^{(k)}_{m,j}(t_{i+1})$, where $\vecG{\Theta}^{(k)}_{m,j}(t_{i+1}) \define [\vecG{\theta}^{(k)}_{m,j,0}(t_{i+1}), \dots, \vecG{\theta}^{(k)}_{m,j,\Xi}(t_{i+1})]$.
Given that the uploading process could be depicted by the series of counters, we further have
$\Pr\Brace{r^{(k)}_{m,j,\xi+1}(t_{i+1})|r^{(k)}_{m,j,\xi}(t_{i})} \sim \vecG{\theta}^{(k)}_{m,j,\xi}(t_{i+1})$ ($\forall \xi=0,\dots,\Xi$).
Then we could express the distribution with a transition matrix as:
\begin{align}
    \vecG{\Theta}^{(k)}_{m,j}(t_{i+1}) \define \hat{\Gamma}^{(k)}_{m,j}\Paren{\Policy(\Stat(t_i)) }\vecG{\Theta}^{(k)}_{m,j}(t_{i}),
\end{align}
where transition matrix $\hat{\Gamma}^{(k)}_{m,j}$ is a \emph{block matrix}, whose element is transition matrix $\Gamma^{(k)}_{m,j,\xi}$ for the distribution on the series of counters. The definition is given as follows.
\begin{align}
    &\vecG{\theta}^{(k)}_{m,j,\xi+1}(t_{i+1}) \define
    \nonumber\\
    &\begin{cases}
        ( \Gamma^{(k)}_{m,j,\xi} \dots \Gamma^{(k)}_{m,j,\xi-N} ) \times \vecG{\theta}^{(k)}_{m,j,\xi-N-1}(t_i), &{\xi \geq N}
        \\
        ( \Gamma^{(k)}_{m,j,\xi} \dots \Gamma^{(k)}_{m,j,1} ) \times \vecG{\theta}^{(k)}_{m,j,0}(t_i), &{\xi < N}
    \end{cases},
\end{align}
where $\vecG{\theta}^{(k)}_{m,j,0}(t_{i}) \sim \text{Bernoulli}(\tilde{\lambda}^{(k)}_{m,j}(t_i))$, and transition matrix $\Gamma^{(k)}_{m,j,\xi}$ is time-invariant and defined as follows.
\begin{align}
    \Gamma^{(k)}_{m,j,\xi} = 
    \begin{bmatrix}
        &0 \\
        &p^{(k)}_{m,j,\xi} &\bar{p}^{(k)}_{m,j,\xi} \\
        &(p^{(k)}_{m,j,\xi})^2 &2(p^{(k)}_{m,j,\xi}\bar{p}^{(k)}_{m,j,\xi}) &(\bar{p}^{(k)}_{m,j,\xi})^2 \\
        % &(p^{(k)}_{m,j,\xi})^3 &3(p^{(k)}_{m,j,\xi})^2(\bar{p}^{(k)}_{m,j,\xi}) &3(p^{(k)}_{m,j,\xi})(\bar{p}^{(k)}_{m,j,\xi})^2 &(p^{(k)}_{m,j,\xi})^3 \\
        &\dots &\dots &\dots &\dots
    \end{bmatrix}^T,
\end{align}
where $p^{(k)}_{m,j,\xi} \define \Pr\{U^{(k)}_{m,j} < (\xi+1) | U^{(k)}_{m,j}>\xi\}$ and $\bar{p}^{(k)}_{m,j,\xi} = 1 - p^{(k)}_{m,j,\xi}$.
And $\hat{\Gamma}^{(k)}_{m,j}$ is an off-diagonal block matrix plus the horizontal part w.r.t $\vecG{\theta}^{(k)}_{m,j,0}$, with other entries as zero.

The second part denotes the state transition on edge servers.
Denote $\Pr\Brace{ Q_{m,j}(t_{i+1})|Q_{m,j}(t_{i}), \mathcal{R}(t_{i}) } \sim \vecG{\nu}_{m,j}(t_i)$. We notice that the job arrival distribution $\vecG{\beta}_{m,j}(t_{i})$ is given by $\mathcal{R}(t_{i})$, and the departure rate in one slot is deterministic as $1/N$.
Thus the expectation of $\vecG{\beta}$ would be always far more smaller than $1$ as composed of all $K$ AP nodes.
We take approximation on $\vec{\beta}$ as Bernoulli distribution in each time slot that $\vecG{\beta}(t_{i,n}) \triangleq [\beta^{(0)}_{m,j}(t_{i,n}), \beta^{(1)}_{m,j}(t_{i,n})]$, where
\begin{align}
    \beta^{(1)}_{m,j}(t_{i,n}) &\define \sum_{k\in\mathcal{K}} \sum_{\xi=0,\dots,\Xi-1} \mathbb{E}[\vecG{\rho}^{(k,+)}_{m,j,\xi}(t_{i,n})]
    \label{eqn_0}
    \\
    \vecG{\rho}^{(k,+)}_{m,j,\xi}(t_{i,n}) &\define \tilde{\Gamma}^{(k)}_{m,j,\xi} \times \vecG{\theta}^{(k)}_{m,j,\xi}(t_{i,n})
    % \label{eqn_1}
    \\
    \beta^{(0)}_{m,j}(t_{i,n}) &= 1-\beta^{(1)}_{m,j}(t_{i,n})
    % \label{eqn_2}
\end{align}
And we could obtain the time-variant transition matrix composed of multiple transition matrix $P_{m,j}(\vecG{\beta}(t_{i,n}))$ in all the time slots in $i$-th interval as follows.
\begin{align}
    \vecG{\nu}(t_{i,n+1}) &= P_{m,j}\Paren{\vecG{\beta}(t_{i,n})} \vecG{\nu}(t_{i,n})
    % \label{eqn_3}
    \\
    \vecG{\nu}(t_{i+1}) &= \prod_{n=0,\dots,N-1} P_{m,j}\Paren{\vecG{\beta}(t_{i,n})} \vecG{\nu}(t_{i}),
    \label{eqn_4}
\end{align}
%FIXME: could/should we give explicit definition for $P_{m,j}$ ?
% where
% \begin{align}
%     \Paren{ P_{m,j}(\vecG{\beta}(t_{i,n})) }_{} \define 
%     \begin{cases}
%         , &\eta_{m,j} = 1
%         \\
%         , &\eta_{m,j} > 1
%     \end{cases}
% \end{align}

However, after the state decomposition, the action space would still be exponentially expanded with respect to the number of APs and edge servers. We could not use traditional \emph{policy iteration} or \emph{value iteration} algorithm \cite{sutton1998introduction} for unacceptable computational complexity.
So in next section, to alleviate curse of dimensionality, we introduce baseline dispatching policy to approximate the value function, and then carry out one-step iteration to obtain a better value function approximation.
%----------------------------------------------------------------------------------------%