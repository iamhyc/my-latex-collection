\section{Distributive Algorithm with Partial Information}
\label{sec:algorithm}

In this section, we shall introduce a novel approximation method to decouple the centralized optimization on the RHS of the Bellman's equations to each AP for arbitrary system state.
Specifically, the decoupling can be achieved via the following two steps:
\begin{enumerate}
    \item We first introduce a baseline policy, use its value function to approximate the value function of the optimal policy $\Policy^*$, and derive the analytical expression of approximate value function in Section \ref{subsec:baseline}.
    \item Based on the approximate value function, an alternative action update algorithm, where a subset of APs are selected to update their dispatching action distributively in each broadcast interval, is proposed in Section \ref{subsec:ap_alg}.
    Moreover, the analytically performance bound is also derived.
\end{enumerate}

\subsection{Baseline Policy and Approximate Value Function}
\label{subsec:baseline}
To alleviate the curse of dimensionality, we first use the baseline policy with fixed dispatching action to approximate value function at the RHS of the Bellman's equations in equation (\ref{eqn:val_f}).
Specifically, the baseline policy is elaborated below.

\begin{policy}[Baseline Policy]
    In the baseline policy $\Baseline$, each AP fixes the target processing edge server for each job type as the previous broadcast interval. Specifically, in the $t$-th broadcast interval,
    \begin{align}
        \Baseline(\Stat(t)) &\define \Bracket{ \Pi_{1}(\Stat_{1}(t)), \dots, \Pi_{K}(\Stat_{K}(t)) },
    \end{align}
    where 
    \begin{align}
        \Pi_{k}(\Stat_{k}(t)) &\define \Brace{
            \omega_{k,j}(t) | \forall j\in\jSpace
        }, \forall k\in\apSet.
    \end{align}
\end{policy}

Let $W_{\Baseline}(\cdot)$ be the value function of the baseline policy, and we shall approximate the value function of the optimal policy $V(\cdot)$ via $\Baseline$, i.e.
{\small
\begin{align}
    V\Paren{\Stat(t+1)} &\approx W_{\Baseline}\Paren{\Stat(t+1)}
    \nonumber\\
    &= \sum_{m\in\esSet,j\in\jSpace}\Brace{
        \sum_{k\in\apSet} \tilde{W}^{\AP}_{k,m,j}(\Stat(t+1))
        \nonumber\\
        &~~~~~~~~~~~~~~~~~~~~~~+\tilde{W}^{\ES}_{m,j}(\Stat(t+1))
    },
\end{align}
}
where $\tilde{W}^{\AP}_{k,m,j}(\Stat(t+1))$ denotes the cost raised by the type-$j$ job which is transmitting from the $k$-th AP to the $m$-th edge server with the baseline policy $\Baseline$ and initial system state $\Stat(t+1)$, and $\tilde{W}^{\ES}_{m,j}(\Stat(t+1))$ denotes the cost raised by the type-$j$ job on the $m$-th server.
Their definitions are given below.
{\small
\begin{align}
    \tilde{W}^{\AP}_{k,m,j} \Paren{\Stat(t+1)} &\define
        \sum_{i=0}^{\infty} \gamma^{i+1} \mathbb{E}^{\Baseline}\Bracket{
            \Inorm{\vec{R}^{(k)}_{m,j}(t+i+1)}
        },
    \\    
    \tilde{W}^{\ES}_{m,j} \Paren{\Stat(t+1)} &\define
        \sum_{i=0}^{\infty} \gamma^{i+1} \mathbb{E}^{\Baseline}\Bracket{
            Q_{m,j}(t+i+1) +
            \nonumber\\
            &~~~~~~~~~~\beta I[Q_{m,j}(t+i+1) = L_{max}]
        }.
\end{align}
}

Moreover, the explicit expression of $\tilde{W}^{\AP}_{k,m,j}(\Stat(t+1))$ and $\tilde{W}^{\ES}_{m,j}(\Stat(t+1))$ are derived in the following lemmas, respectively.

\begin{lemma}[Analytical Expression of $\tilde{W}^{\AP}_{k,m,j}$]
    \label{lemma:w_ap}
    \begin{align}
        &\tilde{W}^{\AP}_{k,m,j}\Paren{\Stat(t+1)} =
        \Inorm{
            \vecG{\Theta}^{(k, \Baseline)}_{m,j}(t+1) \times
            \Bracket{
                \mat{I} - \gamma \Gamma^{(k)}_{m,j}
            }^{-1}
        },
        \label{w_ap}
    \end{align}
    where $\vecG{\Theta}^{(k, \Baseline)}_{m,j}(t)$ and $\Gamma^{(k)}_{m,j}$ are defined below.
    \begin{itemize}
        % \item $\vecG{\Theta}^{(k, \Baseline)}_{m,j}(t) \in \mathbb{R}^{(\Xi+1) \times 1}$ denotes the probability vector under baseline $\Baseline$
        \item {$\vecG{\Theta}^{(k,\Baseline)}_{m,j}(t) \define \Bracket{
            \theta^{(k,\Baseline)}_{m,j}(0,t),
            \theta^{(k,\Baseline)}_{m,j}(1,t),
            \dots,
            \theta^{(k,\Baseline)}_{m,j}(\Xi,t)
            }$},
        where 
        \begin{align}
            \theta^{(k)}_{m,j}(\xi,t) \define 
            \begin{cases}
                \lambda_{k,j} I[\omega_{k,j}(t)=m], & \xi=0
                \\
                \Pr\{R^{(k)}_{m,j}(\xi,t,0) = 1\}, & \text{otherwise}
            \end{cases}
        \end{align}
        % \item $\Gamma^{(k)}_{m,j} \define (\hat{\Gamma}^{(k)}_{m,j})^{t_B}$ where
        \item
        \begin{align}
            \Gamma^{(k)}_{m,j} &\define
            \begin{bmatrix}
                1 & \bar{p}^{(k)}_{m,j,0} &                       &        &                           \\
                  & 0                     & \bar{p}^{(k)}_{m,j,1} &        &                           \\
                  &                       & \ddots                & \ddots &                           \\
                  &                       &                       & \ddots & \bar{p}^{(k)}_{m,j,\Xi-1} \\
                  &                       &                       &        & 0                         \\
            \end{bmatrix}^{t_B}
            \label{eqn:gamma}
        \end{align}
        and $\bar{p}^{(k)}_{m,j,\xi} \define 1 - \Pr\{U^{(k)}_{m,j} < \xi+1 | U^{(k)}_{m,j}>\xi\}$.
    \end{itemize}
    % \begin{align}
    %     \vec{\Theta}_{m,j}^{(k)}(t+1) = {\Gamma}^{(k)}_{m,j}(\Pi_{t,k}) \times \vec{\Theta}_{m,j}^{(k)}(t).
    % \end{align}
\end{lemma}
\begin{proof}
    Please refer to Appendix \ref{append_1}.
\end{proof}

\begin{lemma}[Analytical Expression of $\tilde{W}^{\ES}_{m,j}$]
    \label{lemma:w_es}
    {\small
    \begin{align}
        &\tilde{W}^{\ES}_{m,j}\Paren{\Stat(t+1)}
    = \sum_{i=0,\dots,\frac{\Xi}{T}} \gamma^{i} \mathbb{E}^{\Baseline}[ Q_{m,j}({t+i+1}) ]
    \nonumber\\
    &~~~~~~~~~~~~+ \gamma^{\frac{\Xi}{T}} 
    \vecG{\nu}({t+\frac{\Xi}{T}+1})
    \Paren{
        \mat{I} - \gamma \mat{P}^{\Baseline}_{m,j}(t)
    }^{-1} \vec{g}',
        \label{w_es}
    \end{align}   
    }
    where $\vecG{\nu}_{m,j}(t)$, $\mat{P}_{m,j}(\beta_{m,j}(t))$, $\beta_{m,j}(t)$ and $\vec{g}$ are defined below.
    \begin{itemize}
        \item {\small
        $\vecG{\nu}_{m,j}(t) \define [\Pr\{Q_{m,j}(t)=0\}, \dots, \Pr\{Q_{m,j}(t)=L_{max}\}]$
        }.

        \item $\vec{g} \in \mathbb{R}^{(L_{max}+1) \times 1}$ whose $i$-th element is
        \begin{align}
            [\vec{g}]_{i} = 
            \begin{cases}
                i, & i=0,1,\dots,L_{max}-1
                \\
                L_{max}+\beta, & \text{otherwise}
            \end{cases}.
            \label{eqn:g_vec}
        \end{align}

        \item $\mat{P}^{\Baseline}_{m,j}(t) \in \mathbb{R}^{(L_{max}+1) \times (L_{max}+1)}$ denotes the transition matrix under baseline policy $\Baseline$ whose entries are described in Appendix \ref{append_2}.
    \end{itemize}   
\end{lemma}
\begin{proof}
    Please refer to Appendix \ref{append_2}.
\end{proof}

\subsection{The Distributive Algorithm}
\label{subsec:ap_alg}
Although the optimal value function has been approximated via the baseline policy in the previous part, it is still infeasible for all the APs to solve the RHS of the Bellman's equations in a distributive manner with OSI only.
This is because the evaluation of equation (\ref{w_ap}) and (\ref{w_es}) requires the knowledge of GSI at each AP.
Instead, it is feasible for part of APs to update their dispatching actions distributively and achieve a better performance compared with baseline policy.
Hence, we first define the following sequence of AP subsets, where each subset are selected to update dispatching actions periodically.
\begin{definition}[Subsets of Periodic Actions Update]
    Let $\mathcal{Y}_{1}, \dots, \mathcal{Y}_{N} \subseteq \ccSet$ be a sequence of subset, where each subset satisfies the following constraints
    \begin{align}
        &\bigcup_{n=0,\dots,N-1} \mathcal{Y}_{n} = \apSet
        \\
        \esSet_{y} \cap \esSet_{y'} &=\emptyset, y' \neq y~(\forall y',y \in \mathcal{Y}_{n}).
    \end{align}
\end{definition}
For example, as illustrated in Fig.\ref{fig:conflict}, the AP set $\apSet$ could be decomposed of two subsets as $\set{1,3}$ and $\set{2}$.
Hence, in the $t$-th broadcast interval, the APs in the subset indexed with $n \define t \pmod{N}$ update their dispatching actions, while the other APs keep the same dispatching actions as the previous broadcast interval.
Hence, let
\begin{align}
    \tilde{\mathcal{A}}(t) \define \Brace{
        \tilde{\mathcal{A}}_{y}(t) \define \set{ \tilde{\omega}_{y,j}(t)\in \esSet_{y}|\forall j\in\jSpace } | \forall y\in\mathcal{Y}_{n}
    }
\end{align}
be the aggregation of dispatching actions for the APs in the subset $\mathcal{Y}_{n}$, and
\begin{align}
    \hat{\mathcal{A}}(t) \define \set{\Pi_{y}(\Stat(t)) | \forall y\notin\mathcal{Y}_{n}}
\end{align}
be the aggregation of dispatching actions of the rest APs.
In the $t$-th broadcast interval, the optimization of $\tilde{\mathcal{A}}_{y}(t)$ ($\forall y\in\mathcal{Y}_{n}$) at the RHS of the Bellman's equations can be rewritten as the following problem.
{\small
\begin{align}
    \textbf{P2:}~
    \min_{ \tilde{\mathcal{A}}(t) }
    &\sum_{\Stat(t+1)} \Pr\Brace{
        \Stat(t+1) | \Stat(t), \tilde{\mathcal{A}}(t), \hat{\mathcal{A}}(t)
    } \cdot W_{\Baseline}\Paren{\Stat(t+1)},
\end{align}
}

Moreover, we have the following conclusion on the decomposition of P2.
\begin{lemma}[]
    The optimization problem in P2 can be equivalently decoupled into the following local optimization problem.
    {\small
    \begin{align}
        \textbf{P3:}~
        \min_{ \tilde{\mathcal{A}}_{y}(t) }
        \mathbb{E}_{\set{ \Stat(t+1), \tilde{\mathcal{A}}_{y}(t) }}
        &\sum_{j\in\jSpace,m\in\esSet_{y}} \Brace{
            \tilde{W}^{\AP}_{y,m,j}\Paren{\Stat(t+1)}
            \nonumber\\
            &~~~~~~~~~~~~~~~~~+\tilde{W}^{\ES}_{m,j}\Paren{\Stat(t+1)}
        }.
        \label{eqn:partial}
    \end{align}
    }
\end{lemma}
\begin{proof}
    The proof is delete.
    % We could simply reduce the expression of equation (\ref{w_ap}) and equation (\ref{w_es}) on the RHS of the Bellman's equations based on local OSI, while GSI is not necessary.
    % because the update policy would only change the cost raised on local AP and the corresponding \emph{candidate server set}.
\end{proof}

The optimization of $\tilde{\mathcal{A}}_{y}(t)$ in P3 could be applied by linear search over the solution space of size $|\esSet_{y}|$.
The computational complexity of solving P3 with linear search is $O(J(KM+M))$ for evaluating the objective function in equation (\ref{eqn:partial}).

As a result, the overall algorithm of job dispatching is elaborated in Algorithm \ref{alg_1}.
\begin{algorithm}[h]
    \caption{Online Alternative Actions Update Algorithm}\label{alg_1}
    \DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
    % \KwIn{$\Stat(t), \Delay(t)$}
    % \KwOut{$\tilde{\mathcal{A}}(t)$}

    Initialize $\tilde{\mathcal{A}}(0),\hat{\mathcal{A}}(0)$ with heuristic dispatching actions.\;
    \For{$t=0,1,2,\dots$}{
        $n \gets t \pmod{N}$\;
        \For{$y \in \mathcal{Y}_{n}$}{
            Observe $\Stat_{y}(t)$ after $\mathcal{D}_{y}(t)$.\;
            $\tilde{\mathcal{A}}_{y}(t+1) \gets$ solving P3 with $\Stat_{y}(t), \mathcal{D}_{y}(t)$\;
            $\hat{\mathcal{A}}_{y}(t+1) \gets \tilde{\mathcal{A}}_{y}(t)$ \;
        }
        % \Return $\tilde{\mathcal{A}}(t+1)$\;
    }
\end{algorithm}
% At the $1$-st broadcast time slot when $t=0$, the APs in set $\mathcal{Y}_{1}$ and the edge servers in the corresponding \emph{candidate server set}s of APs in $\mathcal{Y}_{1}$, should broadcast their LSI (including the heuristic actions)

Finally, we have the following conclusion on the performance of the above proposed algorithm.

\begin{lemma}[Performance Guarantee]
    \label{lemma:perform}
    Based on the proposed actions update algorithm, in the $t$-th broadcast interval, we denote the joint dispatching actions as
    \begin{align}
        \vecG{\pi}^{(t)}(\Stat(t)) = \tilde{\mathcal{A}}(t) \cup \hat{\mathcal{A}}(t),
    \end{align}
    where $\vecG{\pi}^{(t)}$ is the stationary policy obtained by solving problem P3 ($\forall \Stat$).
    Let $W_{\vecG{\pi}^{(t)}}(\cdot)$ be the value function of the policy $\vecG{\pi}^{(t)}$
    \begin{align}
        W_{\vecG{\pi}^{(t)}}(\Stat) \define
        \sum_{t'=1}^{\infty} \gamma^{t'-1} \mathbb{E}^{ \vecG{\pi}^{(t)} } \Bracket{
            g\Paren{\Stat(t'), \vecG{\pi}^{(t)}(\Stat(t'))} | \Stat(1)=\Stat
        },
    \end{align}
    % The performance of the baseline policies is upper bounded by the optimal solution $\Policy^*$ when considering the original Bellman's equations.
    we have
    \begin{align}
        V_{\Policy^*}\Paren{\Stat}
        \leq \lim_{t\to\infty} W_{\vecG{\pi}^{(t)}}\Paren{\Stat}
        \leq W_{\Baseline}\Paren{\Stat},
        \forall \Stat.
    \end{align}
\end{lemma}
\begin{proof}
    \fixit{
        Policy Improvement in \cite{dp-control}.
        \blindtext
    }
\end{proof}

%----------------------------------------------------------------------------------------%
%----------------------------------------------------------------------------------------%