\section{Decentralized Algorithm with Partial Information}
\label{sec:algorithm}

\subsection{Low-Complexity Solution Framework}
In this part, we shall introduce a novel approximation method to decouple the optimization on the right-hand-side of the Bellman's equations for arbitrary system state.
Specifically, the decoupling can be achieved via the following two steps:
\begin{enumerate}
    \item we first introduce a baseline policy and use its value function to approximate the value function of the optimal policy;
    \item based on the approximate value function, only a {subset of APs} are scheduled for update the dispatching action in each broadcast interval.
\end{enumerate}
First of all, the baseline policy is defined as follows.
\begin{definition}[Baseline Policy]
    In the baseline policy $\Baseline$, each AP fixes the target processing edge server for each job type as the previous broadcast interval. Specifically,
    \begin{align}
        \Baseline(\Stat(t)) &\define \Bracket{ \Pi_{1}(\Stat_{1}(t)), \dots, \Pi_{K}(\Stat_{K}(t)) },
    \end{align}
    where 
    \begin{align}
        \Pi_{k}(\Stat_{k}(t)) &\define \Brace{
            \omega_{k,j}(t) | \forall j\in\jSpace
        }, \forall k\in\apSet.
    \end{align}
\end{definition}

Thus, we shall approximate the value function of the optimal policy as
\begin{align}
    &V\Paren{\Stat(t+1)} 
    \nonumber\\
    \approx&~ W^{\dagger}_{\Baseline(\Stat(t))} \Paren{\mathcal{R}(t+1)} + W^{\ddagger}_{\Baseline(\Stat(t))}\Paren{\mathcal{Q}(t+1)}
    \nonumber\\
    =& \sum_{j\in\jSpace}\Brace{
        \sum_{k\in\apSet}\sum_{m\in\esSet}\Bracket{ \sum_{i=0}^{\infty} \gamma^{i+1} \Inorm{\vec{R}^{(k)}_{m,j}(t+i+1)} }
        \nonumber\\
        & + \sum_{m\in\esSet} \Bracket{ \sum_{i=0}^{\infty} \gamma^{i+1} Q_{m,j}(t+i+1) }
    }.
\end{align}

Moreover, we define the following sequence of AP subsets, where APs of each subset are selected to update dispatching strategy periodically.
\begin{definition}[Subsets of Periodic Strategy Update]
    Let $\mathcal{Y}_{1}, \dots, \mathcal{Y}_{N} \subseteq \ccSet$ be a sequence of subset, where each subset (say the $n$-th subset) satisfies the following constraints
    \begin{align}
        &\bigcup_{n=1,\dots,N} \mathcal{Y}_{n} = \apSet
        \\
        \ccSet_{y} \cap \ccSet_{y'} &=\emptyset, \forall y' \neq y \in \mathcal{Y}_{n}~(n=1,\dots,N).
    \end{align}
\end{definition}

Hence, the proposed dispatching policy, based on the one-step policy iteration on the baseline policy, is defined as follows.
\begin{definition}[The Dispatching Policy]
    In the $t$-th broadcast, the AP subset indexed with $n \equiv t \pmod{N} + 2$ shall update their policies.
    Specifically, the dispatching policies of the APs outside the subset $\mathcal{Y}_{n}$ is given by
    \begin{align}
        \Omega_{y}( \Stat_{y}(t),\Delay_{y}(t) ) \define \set{ \omega_{k,j}(t) | \forall j\in\jSpace }, \forall y\notin\mathcal{Y}_{n}.
    \end{align}
    Moreover, the dispatching policies of the APs in the subset $\mathcal{Y}_{n}$ is given by solving the following Problem \ref{problem_2}.
\end{definition}

\begin{problem}
    In the $t$-th broadcast interval, the $y$-th AP in the set $\mathcal{Y}_{n}$ ($n \equiv t \pmod{N} + 1$) shall solve the following problem to update its policy.
    {\tiny
    \begin{align}
        &\min_{ \Omega_{y}(\Stat_{y}(t), \Delay_{y}(t)) }
        \sum_{\Stat(t+1)} \Bracket{
            W^{\dagger}_{\Baseline(\Stat(t))} \Paren{\mathcal{R}(t+1)} + W^{\ddagger}_{\Baseline(\Stat(t))}\Paren{\mathcal{Q}(t+1)}
        } \times
        \nonumber\\
        &\Pr\Brace{
            \Stat(t+1) | \Stat(t), \Omega_{y}(\Stat_{y}(t), \Delay_{y}(t)), \set{\omega_{y',j}(t)|\forall y' \neq y \in\apSet, j\in\jSpace}
        }
    \end{align}
    }
    \label{problem_2}
\end{problem}

\subsection{The Decentralized Algorithm}
Then, we could propose the algorithm to solve the decentralized problem for each AP.
The decentralized problem for the AP is defined as follows.
% The optimization problem at right-hand side of approximate Bellman's Equation is given as follows.
\begin{problem}[Decentralized Approximated Optimization Problem for AP]
    For the $k$-th AP with baseline policy $\Pi_{k}(t)$:
    {\small
    \begin{align}
        \min_{\Pi_{k}(t)} \mathbb{E}_{\set{ \Stat(t+1), \Pi_{k}(t) }} \Bracket{
            W^{\AP}_{\Pi_{k}(t)}\Paren{\mathcal{R}(t+1)} + W^{\ES}_{\Pi_{k}(t)}\Paren{\mathcal{Q}(t+1)}
        }.
        \label{eqn:partial}
    \end{align}
    }
\end{problem}

The expected value function $W^{\AP}_{\Pi_{k}(t)}(\mathcal{R}(t+1))$ is easily obtained by calculating the following equation.
\begin{align}
    W^{\AP}_{\Pi_{k}(t)}\Paren{\mathcal{R}(t+1)} &= \sum_{j\in\jSpace}\sum_{k\in\ccSet_{k}}\sum_{m\in\esSet_{k}}
    \nonumber\\
    &\Inorm{
        \Paren{ 1 - \gamma ({\Gamma}^{(k)}_{m,j})^{N} }^{-1} \hat{\vecG{\Theta}}^{(k)}_{m,j}(t+1)
    }.
    \label{w_ap}
\end{align}
% where $\hat{\Gamma}^{(k)}_{m,j} \define (\Gamma^{(k)}_{m,j})^{N}$.

The expression for expected value function $W^{\ES}_{\Pi_{k}(t)}(\mathcal{Q}(t+1))$ is little more complex compared to Eqn.(\ref{w_ap}).
It is affected with both arrival process under dispatching policy and last queue state.
However, we notice that the arrival process would be stationary after the maximum uploading time under the stationary baseline policy and the relationship between APs and edge server could be decoupled.
\begin{align}
    &W^{(k)}_{\Baseline(t)}\Paren{\mathcal{Q}(t+1)}
    = \sum_{j\in\jSpace}\sum_{m\in\esSet_{k}}\sum_{i=0,\dots,\frac{\Xi}{T}} \gamma^{i} \mathbb{E}[ Q_{m,j}({t+i+1}) ]
    \nonumber\\
    &~~~~~~~~~~~~~~~~~~~+ \gamma^{\frac{\Xi}{T}} \Paren{ \mat{I} - \gamma \hat{\mat{P}}_{m,j}(\tilde{\beta}_{m,j}) }^{-1} \vecG{\nu}({t+\frac{\Xi}{T}+1}) \vec{g}',
\end{align}
where the $i$-th element of vector $\vec{g}$ denotes the cost of server as $Q_{m,j}(t)$;
\begin{align}
    \hat{\mat{P}}_{m,j}(\tilde{\beta}_{m,j}) \define \prod_{n=0,\dots,N-1} \mat{P}_{m,j}(\tilde{\beta}_{m,j})
\end{align}
% the probability distribution of $Q_{m,j}({t+i+1})$ is denoted by $\vecG{\nu}({t+i+1})$ which is obtained by calculating equation Enq. (\ref{eqn_0}) - Eqn. (\ref{eqn_4}) ($\forall i=0,\dots,\frac{\Xi}{T}$);
where $\tilde{\beta}_{m,j}$ is the arrival distribution under baseline policy $\Pi(t)$ (on $m$-th ES with type $j$ jobs)
\begin{align}
    \tilde{\beta}_{m,j} &\define \sum_{k\in\apSet} \tilde{\lambda}^{(k)}_{m,j} \times \sum_{\xi=0,\dots,\Xi-1} \Pr\{ \xi<U_{k,m}\leq(\xi+1) \}
        \nonumber\\
    ~~~~&= \sum_{k\in\apSet} \tilde{\lambda}^{(k)}_{m,j}
\end{align}

The details of the procedure is elaborated as follows.
\begin{itemize}
    \item Choose the initial \emph{system dispatching policy} start with some heuristic policy.
    \item At the first broadcast time slot when $t=0$, the APs and edge servers in \emph{conflict AP set} and \emph{candidate server set}, respectively, of the $1$-st AP would broadcast their LSI (including the heuristic policies);
    \item The $1$-st AP would receive the OSI at the $D_1(1)$ time slots in the first broadcast interval, and then it updates its policy $\Omega_{1}(\Stat_{1}(1))$ by solving Eqn.(\ref{eqn:sp_0}) for the $1$-st AP.
    The $1$-st AP would keep with this policy until it receives the OSI again.
    \item At the second broadcast time slot when $t=1$, the OSI of the $2$-nd AP would be broadcast;
    the $1$-st AP would broadcast $\Omega_{1}(\Stat_{1}(1))$ if it's in the \emph{conflict AP set} of the $2$-nd AP;
    \item Similarly, in the $t$-th broadcast interval the AP indexed with $k' \equiv (t + 1)\mod{K}$ would receive the OSI $D_{k'}(t)$ time slots later, and then it updates its policy $\Omega_{k'}(t+1)$ by solving Eqn.(\ref{eqn:sp_0}) for the $k'$-th AP.
\end{itemize}

% \begin{lemma}
%     The optimized policy solved as $\Baseline(t)$ for next stage, is better than $\Baseline(t-1), \dots, \Baseline(1)$ when considering the \emph{approximated value function} defined above.
%     \\
%     Moreover, the performance of the series of the baseline policies is upper bounded by the optimal solution $\Policy^*$ when considering the original Bellman's equation.
%     $$
%         V_{\Omega^*}\Paren{\Stat(t)}
%         \leq W_{\tilde{\Policy}(t)}\Paren{\Stat(t)}
%         \leq W_{\tilde{\Policy}(t-1)}\Paren{\Stat(t)}
%     $$
% \end{lemma}
% \begin{proof}
    
% \end{proof}

%----------------------------------------------------------------------------------------%
\delete{v11}{
    % [\IF, \ENDIF], [\FOR, \TO, \ENDFOR], [\WHILE, \ENDWHILE], \STATE, \AND, \TRUE
    % \begin{algorithm}[H]
    %     \caption{Online Iterative Policy Improvement Algorithm}
    %     \begin{algorithmic}[1]
    %         \STATE $t = 0$
    %         \FOR{$t = 1,2,\dots$}
    %             \STATE Evaluate $\Omega_0$ in \textbf{P1} according to Eqn. (\ref{sp1})
    %             \FOR{$k \in \mathcal{K}$}
    %                 \STATE fix policy $\vec{\Omega}^{(k)}(t) \forall k' < k$
    %                 \STATE Evaluate $k$-th AP Local Policy $\tilde{\Omega}_k$ in \textbf{Pk} according to Eqn. (\ref{sp2})
    %             \ENDFOR
    %         \ENDFOR
    %     \end{algorithmic}
    % \end{algorithm}
}
%----------------------------------------------------------------------------------------%