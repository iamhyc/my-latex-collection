\section{LOW-COMPLEXITY SOLUTION}
In this section, we introduce a heuristic dispatching algorithm as the baseline policy, whose value function could be derived analytically.
Then the right-hand side of Bellman's Equation turns into simple minimization problem over the control policy. The sub-optimal policy could be obtained by evaluation of approximated value function on right-hand side of Bellman's Equation with one-step iteration. And thus the approximated value function is the upper bound of the original optimal policy.

Moreover, as the system state is partially presented as $\Obsv({t-1})$ and $\Obsv({t})$ separated by the \brdelay~$\mathcal{D}(T)$, different APs would have isolated acknowledge on the joint policy and transition function due to different $D_k(t)$ ($\forall k\in\apSet$).
Thus the joint optimization over the components of $\Policy(\Stat({t}))$ is impossible.
To overcome the gap, we leverage the above global problem structure, but solve it with iterative policy update algorithm.

\delete{v4}{
    \begin{itemize}
        \item The updated global system state information presents at different time slots for different APs.
        \item The delay-aware algorithm proposed allow APs update their policy iteratively in a polling manner, while they all solve their own approximated Bellman's equation.
        There is at most one AP updates its policy at one time while the other APs keep on with the last policy.
        For example, in the $t$-th broadcast interval, the $k$-th AP would update its policy $\tilde{\Omega}_{k}(\Obsv({t}))$ according to the $k$-th sub-problem while the other AP nodes would keep with the previous policy $\tilde{\Omega}_{k'}(\Obsv({t-1}))$($\forall k' \neq k \in\apSet$).
        \item This algorithm would lead to performance improvement guarantee with global optimality as upper bound.
    \end{itemize}
}

\subsection{Baseline Dispatching Policy}
The baseline dispatching policy is adopted to obtain an approximation of value function. The policy on each AP nodes is state-invariant, which is denoted as:
\begin{align}
    \BaseLine({t}) \define \Bracket{\Pi_1({t}), \Pi_2({t}), \dots, \Pi_K({t})},
\end{align}
where $\Pi_k({t}) \define \set{\pi^{(k)}_{m,j}({t}) | \forall m\in\esSet,\forall j\in\jSpace}$ denotes ($\forall k\in\apSet$) the action set resembling the definition in Eqn. (\ref{def_action}).
More specifically, $\BaseLine(t)$ is assumed as the stationary baseline policy adopted globally after the $t$-th \brpoint~and the baseline policies are different in each broadcast interval.

According to the additive structure of cost function, we substitute the transition function plus value function in Eqn. (\ref{sp_0}) with two linearly divided sections as $W^{\AP}(\mathcal{R}({t+1}))$ and $W^{\ES}(\mathcal{Q}({t+1}))$ under baseline policy $\BaseLine({t})$:
\begin{align}
    &V(\Stat({t})) = g(\Stat({t})) +
    \nonumber\\
    &~~~~~~\gamma \min_{\BaseLine({t})} \Bracket{ W^{\AP}_{\BaseLine({t})}\Paren{\mathcal{R}({t+1})} + W^{\ES}_{\BaseLine({i})}\Paren{\mathcal{Q}({t+1})} },
\end{align}
where $W^{\AP}_{\BaseLine({t})}(\cdot)$ and $W^{\ES}_{\BaseLine({t})}(\cdot)$ denote the split \emph{expected value function} over AP nodes and ES nodes, respectively, $\Policy(\Stat({t})) = \tilde{\Omega}({t-1}), \BaseLine({t})$. The split expected value functions are defined as follows.
\begin{align}
    W^{\AP}_{\BaseLine({t})}\Paren{\mathcal{R}({t})}
        &\define \sum_{k\in\apSet}\sum_{m\in\esSet}\sum_{j\in\jSpace}
        % \nonumber\\
        \mathbb{E}_{\{\BaseLine({t}), \vec{R}^{(k)}_{m,j}({t})\}}\Bracket{
            \sum_{i=0}^{\infty} \gamma^{i} \Inorm{\vec{R}^{(k)}_{m,j}({t+i})}
        }
    \\
    W^{\ES}_{\BaseLine({t})}\Paren{\mathcal{Q}({t})}
        &\define \sum_{m\in\esSet}\sum_{j\in\jSpace}
        % \nonumber\\
        \mathbb{E}_{\{\BaseLine({t}), Q_{m,j}({t})\}}\Bracket{
            \sum_{i=0}^{\infty} \gamma^{i} l_{m,j}({t+i})
        }.
\end{align}
        
The decoupled value functions for AP and ES nodes is obtained with an approximated form under the baseline policy. We use the same baseline policy to evaluate both low-complexity policy performance in Eqn. (\ref{sp_0}).

The expected value function $W^{\AP}(\mathcal{R}({t+1}))$ under baseline $\vecG{\Pi}({t})$ for AP nodes is easily obtained by calculating by the following equation:
\begin{align}
    &W^{\AP}_{\BaseLine({t})}\Paren{\mathcal{R}({t+1})} = \sum_{k\in\apSet}\sum_{m\in\esSet}\sum_{j\in\jSpace}
    % \nonumber\\
    \Inorm{
        \Paren{1 - \gamma \hat{\Gamma}^{(k)}_{m,j}}^{-1}
        \times \vecG{\Theta}^{(k)}_{m,j}({t+1})
    },
    \label{w_ap}
\end{align}
where $\hat{\Gamma}^{(k)}_{m,j} \define (\Gamma^{(k)}_{m,j})^{N}$, $\Inorm{\cdot}$ denotes the sum of absolute value of each entry of the vector.
\delete{v4}{And the transition function $\Pr\{ \mathcal{R}({t+1})|\mathcal{R}({t}) \}$ is embedded with its distributions $\vecG{\Theta}^{(k)}_{m,j}({t+1})$ in the equation.}

The expected value function $W^{\ES}(\mathcal{Q}({t+1}))$ under baseline $\vecG{\Pi}({t})$ for ES nodes is little more complex compared to Eqn. (\ref{w_ap}).
The ES node transition is affected with both arrival process under dispatching policy and last queue state, and we reduce the states expression by taking average over the uploading process together with the transition function expression.
\begin{align}
    &W^{\ES}_{\BaseLine({t})}\Paren{\mathcal{Q}({t+1})}
    \nonumber\\
    =& \sum_{m,j}\sum_{i=0,\dots,\frac{\Xi}{T}} \gamma^{i} \mathbb{E}[ l_{m,j}({t+i+1}) ]
    \nonumber\\
    &~~~~~~~~+ \gamma^{\frac{\Xi}{T}} \Paren{ \mat{I} - \gamma \hat{\mat{P}}_{m,j}(\tilde{\beta}_{m,j}) }^{-1} \vecG{\nu}({t+\frac{\Xi}{T}+1}) \vec{g}',
\end{align}
where $\hat{\mat{P}}_{m,j}(\tilde{\beta}_{m,j}) \define \prod_{n=0,\dots,N-1} \mat{P}_{m,j}(\tilde{\beta}_{m,j})$;
the probability distribution of $l_{m,j}({t+i+1})$ is denoted by $\vecG{\nu}({t+i+1})$ which is obtained by calculation over equation Enq. (\ref{eqn_0}) - Eqn. (\ref{eqn_4}) ($\forall i=0,\dots,\frac{\Xi}{T}$);
the $i$-th element of vector $\vec{g}$ denotes the cost of server as $l_{m,j}(t)$;
$\tilde{\beta}_{m,j}$ is the arrival distribution under baseline policy $\Pi({t})$ (on $m$-th ES with $j$-type job), where:
\begin{align}
    \tilde{\beta}_{m,j} &\define \sum_{k\in\apSet} \tilde{\lambda}^{(k)}_{m,j} \times \sum_{\xi=0,\dots,\Xi-1} \Pr\{ \xi<U_{k,m}\leq(\xi+1) \}
        \nonumber\\
    ~~~~&= \sum_{k\in\apSet} \tilde{\lambda}^{(k)}_{m,j}
\end{align}
%----------------------------------------------------------------------------------------%

\subsection{The Distributed Algorithm}
\fixit{
    \begin{itemize}
        \item We leverage the global optimization problem, while the presence of $\mathcal{D}(t)$ doesn't matter in our algorithm;
        \item We propose one iterative update algorithm (elaboration needed);
        \item The performance bound is guranteed, and always better compared with last stage.
    \end{itemize}
}

The optimization problem at right-hand side of approximate Bellman's Equation is given as follows.
\begin{align}
    \min_{\BaseLine({t})} W^{\AP}_{\BaseLine({t})}\Paren{\mathcal{R}({t+1})} + W^{\ES}_{\BaseLine({t})}\Paren{\mathcal{Q}({t+1})}
    \label{eqn_opt}
\end{align}

Then we introduce the iterative policy update algorithm, which optimize the 
with improved baseline policy in each interval as performance guarantee.
\begin{itemize}
    \item Choose initial policy $\Policy(\Stat(0)) = \BaseLine(0)$ as start, which is the \emph{local greedy policy};
    \item In $t\in[0, 1)$, the AP node indexed with $1$ would receive the broadcast information after $D_1(1)$ and updates the global information; then it solves the optimization problem in Eqn. (\ref{eqn_opt}) with only $\Pi_{1}(1)$ changes in $\Policy(\Stat(1)) = \BaseLine(1) = [\Pi_{1}(1), \Pi_{2}(0), \dots, \Pi_{K}(0)]$;
    \item At $t=1$, the AP node indexed with $1$ would broadcast its policy $\Pi_{1}(1)$ together with the state information;
    \item In $t\in[1, 2)$, the AP nodes indexed with $2$ would receive the broadcast information and previous global policy $\Policy(\Stat(1))$; then it repeats the procedure what the $1$-st AP did;
    \item Then generally, in $t\in[{i}, {i+1})$, the AP nodes indexed with $(k \mod K)+1$ would update its own policy following the above procedure.
\end{itemize}

\begin{lemma}
    The optimized policy solved as $\BaseLine(t)$ (for next stage), is better than $\BaseLine(t-1), \dots, \BaseLine(1)$ when considering the \emph{approximated value function} defined above.
    \\
    Moreover, the performance of the series of the baseline policies is upper bounded by the optimal solution $\Policy^*$ when considering the original Bellman's equation.
\end{lemma}
\begin{proof}
    \fixit{
        The proof is illustrated into two parts.
        \begin{itemize}
            \item $\BaseLine(t)$ is better than the previous baseline policy, \emph{with the latest start} $\Obsv(t+1)$; the other previous policies were determined from the different start state;
            \item Upper bound is easy to prove with one-step iteration that:
            $$
                V_{\Omega^*}\Paren{\Stat(t)}
                    \leq W_{\BaseLine(t)}\Paren{\Stat(t)}
                    \leq W_{\BaseLine(t-1)}\Paren{\Stat(t)}
                    \leq \dots \leq W_{\Pi(1)}\Paren{\Stat(t)}
            $$
            but I doube this could not stand, cause the stationary policy is also affected by \emph{start state} and \emph{random broadcat delay}.
        \end{itemize}
    }
\end{proof}
% [\IF, \ENDIF], [\FOR, \TO, \ENDFOR], [\WHILE, \ENDWHILE], \STATE, \AND, \TRUE
% \begin{algorithm}[H]
%     \caption{Online Iterative Policy Improvement Algorithm}
%     \begin{algorithmic}[1]
%         \STATE $t = 0$
%         \FOR{$t = 1,2,\dots$}
%             \STATE Evaluate $\Omega_0$ in \textbf{P1} according to Eqn. \ref{sp1}
%             \FOR{$k \in \mathcal{K}$}
%                 \STATE fix policy $\vec{\Omega}^{(k)}(t) \forall k' < k$
%                 \STATE Evaluate $k$-th AP Local Policy $\tilde{\Omega}_k$ in \textbf{Pk} according to Eqn. \ref{sp2}
%             \ENDFOR
%         \ENDFOR
%     \end{algorithmic}
% \end{algorithm}

\delete{v4}{
    \begin{lemma}
        The above algorithm would have performance improvement in each interval with baseline policy guarantee, and at least achieve local optimal.
    \end{lemma}
    \begin{proof}
        The improved policy in last interval becomes the baseline policy in next interval.
        So, in each stage, it resembles the one-step Policy Improvement step in classical \emph{Policy Iteration} algorithm, and each step with different value function under baseline policy on the right-hand side.
        
        The insight exists that: due to the uncontrollable delay on policy in each stage, the chosen time-invariant baseline policy actually would; so the improved policy is the baseline of next stage, and thus forms the online policy iteration.
        discounted sum-up cost, the future states near the current state counts more and the baseline policy (which is time-invariant) would variant in each interval.
    \end{proof}
}

\subsection{Reinforcement Learning}
Due to the fact that the statistical parameters of job arrival distribution $\lambda_{k,j}$, uploading time distribution $U_{k,m,j}$ and processing time distribution $C_{m,j}$ are unknown at the first time, we adopt reinforcement learning algorithm to learn the parameters.
%----------------------------------------------------------------------------------------%