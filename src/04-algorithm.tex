\section{Decentralized Algorithm with Partial Information}
\label{sec:algorithm}

In this part, we shall introduce a novel approximation method to decouple the optimization on the RHS of the Bellman's equations for arbitrary system state.
Specifically, the decoupling can be achieved via the following two steps:
\begin{enumerate}
    \item We first introduce a baseline policy and use its value function to approximate the value function of the optimal policy in Section \ref{subsec:baseline};
    \item Based on the approximate value function, an alternative action update algorithm, where a subset of APs are selected to update their dispatching action distributively in each broadcast interval, is proposed in Section \ref{subsec:ap_alg}.
    Moreover, the analytically performance bound is also derived.
\end{enumerate}

\subsection{Baseline Police and Approximate Value Function}
\label{subsec:baseline}
To alleviate the curse of dimensionality, we first use the baseline policy with fixed dispatching action to approximate value function at the RHS of the Bellman's equations in equation (\ref{eqn:val_f}).
Specifically, the baseline policy is elaborated below.

\begin{policy}[Baseline Policy]
    In the baseline policy $\Baseline$, each AP fixes the target processing edge server for each job type as the previous broadcast interval. Specifically, in the $t$-th broadcast interval,
    \begin{align}
        \Baseline(\Stat(t)) &\define \Bracket{ \Pi_{1}(\Stat_{1}(t)), \dots, \Pi_{K}(\Stat_{K}(t)) },
    \end{align}
    where 
    \begin{align}
        \Pi_{k}(\Stat_{k}(t)) &\define \Brace{
            \omega_{k,j}(t) | \forall j\in\jSpace
        }, \forall k\in\apSet.
    \end{align}
\end{policy}

Thus, we shall approximate the value function of the optimal policy as
\begin{align}
    V\Paren{\Stat(t+1)} &\approx W_{\Baseline}\Paren{\Stat(t+1)}
    \nonumber\\
    &= \sum_{j\in\jSpace}\Brace{
        \sum_{k\in\apSet}\sum_{m\in\esSet} \tilde{W}^{\AP}_{k,m,j}(\Stat(t+1))
        \nonumber\\
        &~~~~~~~~~~~~~~+\sum_{m\in\esSet} \tilde{W}^{\ES}_{m,j}(\Stat(t+1))
    }
\end{align}
where $\tilde{W}^{\AP}_{k,m,j}(\Stat(t+1))$ and $\tilde{W}^{\ES}_{m,j}(\Stat(t+1))$ denote the cost raised by the type-$j$ job on the $k$-th AP to the $m$-th server and $m$-th edge servers with the baseline policy $\Baseline$ and initial system state $\Stat(t+1)$, respectively.
Their definitions are given below.
{\small
\begin{align}
    \tilde{W}^{\AP}_{k,m,j} \Paren{\Stat(t+1)} &\define
        \sum_{i=0}^{\infty} \gamma^{i+1} \mathbb{E}^{\Baseline}\Bracket{
            \Inorm{\vec{R}^{(k)}_{m,j}(t+i+1)}
        },
    \\    
    \tilde{W}^{\ES}_{m,j} \Paren{\Stat(t+1)} &\define
        \sum_{i=0}^{\infty} \gamma^{i+1} \mathbb{E}^{\Baseline}\Bracket{
            Q_{m,j}(t+i+1) +
            \nonumber\\
            &~~~~~~~~~~\beta I[Q_{m,j}(t+i+1) = L_{max}]
        }.
\end{align}
}

% To efficiently express the state transition, we further elaborate the distribution probability of the state vector as the corresponding probability vector, and perform the state transition with the transition matrix.
% Specifically, the definitions of symbols in the remaining parts is given in the Appendix \ref{apped_1} and the calculation of equation (\ref{eqn:partial}) is elaborated below.
Moreover, the explicit expression of $\tilde{W}^{\AP}_{k,m,j}(\Stat(t+1))$ and $\tilde{W}^{\ES}_{m,j}(\Stat(t+1))$ are derived in the following lemmas, respectively.

\begin{lemma}[Analytical Expression of $\tilde{W}^{\AP}_{k,m,j}$]
    \begin{align}
        &\tilde{W}^{\AP}_{k,m,j}\Paren{\Stat(t+1)} =
        \Inorm{
            \Bracket{
                \vecG{\Theta}^{(k, \Baseline)}_{m,j}(t+1)
            }'
            \Bracket{
                \mat{I} - \gamma \Gamma^{(k)}_{m,j}
            }^{-1}
        },
        \label{w_ap}
    \end{align}
    where the notations of $\vecG{\Theta}^{(k, \Baseline)}_{m,j}(t)$ and $\Gamma^{(k)}_{m,j}$ are defined below.
    \begin{itemize}
        \item $\vecG{\Theta}^{(k, \Baseline)}_{m,j}(t) \in \mathbb{R}^{(\Xi+1) \times 1}$ denotes the probability vector under baseline $\Baseline$
        \begin{align}
            \vecG{\Theta}^{(k)}_{m,j}(t) &\define
            \Bracket{ \theta^{(k)}_{m,j,0}(t), \theta^{(k)}_{m,j,1}(t), \dots, \theta^{(k)}_{m,j,\Xi}(t) },
        \end{align}
        where 
        \begin{align}
            \theta^{(k)}_{m,j,\xi}(t) \define 
            \begin{cases}
                \lambda_{k,j} I[\omega_{k,j}(t)=m], & \xi=0
                \\
                \Pr\{R^{(k)}_{m,j,\xi}(t,0) = 1\}, & \xi=1,\dots,\Xi
            \end{cases}
        \end{align}
        \item $\Gamma^{(k)}_{m,j} \in \mathbb{R}^{(\Xi+1) \times (\Xi+1)}$ denotes the transition matrix which is given as $\Gamma^{(k)}_{m,j} \define (\hat{\Gamma}^{(k)}_{m,j})^N$ where
        \begin{align}
            \hat{\Gamma}^{(k)}_{m,j} &\define
            \begin{bmatrix}
                1 & \bar{p}^{(k)}_{m,j,0} &                       &        &                           \\
                  & 0                     & \bar{p}^{(k)}_{m,j,1} &        &                           \\
                  &                       & \ddots                & \ddots &                           \\
                  &                       &                       & \ddots & \bar{p}^{(k)}_{m,j,\Xi-1} \\
                  &                       &                       &        & 0                         \\
            \end{bmatrix},
        \end{align}
        and $\bar{p}^{(k)}_{m,j,\xi}$ denotes the job offloading probability ($\forall \xi=0,\dots,\Xi$)
        \begin{align}
            \bar{p}^{(k)}_{m,j,\xi} &\define 1 - \Pr\{U^{(k)}_{m,j} < (\xi+1) | U^{(k)}_{m,j}>\xi\}.
        \end{align}
    \end{itemize}
    % \begin{align}
    %     \vec{\Theta}_{m,j}^{(k)}(t+1) = {\Gamma}^{(k)}_{m,j}(\Pi_{t,k}) \times \vec{\Theta}_{m,j}^{(k)}(t).
    % \end{align}
\end{lemma}
\begin{proof}
    Please refer to Appendix \ref{append_1}.
\end{proof}

\begin{lemma}[Analytical Expression of $\tilde{W}^{\ES}_{m,j}$]
    {\small
    \begin{align}
        &\tilde{W}^{\ES}_{m,j}\Paren{\Stat(t+1)}
        = \sum_{i=0,\dots,\frac{\Xi}{T}} \gamma^{i} \mathbb{E}^{\Baseline}[ Q_{m,j}({t+i+1}) ]
        \nonumber\\
        &~~~~~~~~~~~~+ \gamma^{\frac{\Xi}{T}} 
        \vecG{\nu}({t+\frac{\Xi}{T}+1})
        \Paren{
            \mat{I} - \gamma \mat{P}_{m,j}(\beta_{m,j}(t))
        }^{-1} \vec{g}',
        \label{w_es}
    \end{align}   
    }
    where the notations of $\vecG{\nu}_{m,j}(t)$, $\mat{P}_{m,j}(\beta_{m,j}(t))$, $\beta_{m,j}(t)$ and $\vec{g}$ are defined below.
    \begin{itemize}
        \item $\vecG{\nu}_{m,j}(t) \in \mathbb{R}^{(L_{max}+1) \times 1}$ denotes the probability vector where
        \begin{align}
            \vecG{\nu}_{m,j}(t) \define [\Pr\{Q_{m,j}(t)=0\}, \dots, \Pr\{Q_{m,j}(t)=L_{max}\}].
        \end{align}

        \item $\vec{g} \in \mathbb{R}^{(L_{max}+1) \times 1}$ whose $i$-th element is
        \begin{align}
            [\vec{g}]_{i} = 
            \begin{cases}
                i, & i=0,1,\dots,L_{max}-1
                \\
                L_{max}+\beta, & \text{otherwise}
            \end{cases}.
        \end{align}

        \item $\mat{P}_{m,j}(\beta_{m,j}(t)) \in \mathbb{R}^{(L_{max}+1) \times (L_{max}+1)}$ denotes the transition matrix given the average job arrival rate $\beta_{m,j}(t)$ under baseline policy $\Baseline(\Stat(t))$ where
        \begin{align}
            \beta_{m,j}(t) = \sum_{k\in\apSet} \lambda_{k,j} I[\omega_{k,j}(t)=m].
        \end{align}
        The entries of the transition matrix $\mat{P}_{m,j}$ are provided by table $\ref{tab:matP}$ in Appendix \ref{append_1}.
    \end{itemize}

    % The expression for expected value function is little more complex compared to equation (\ref{w_ap}).
    % The transition matrix for state transition is affected by the baseline policy and the system states of APs.
    % However, we notice that under the fixed baseline policy, the arrival process on edge servers would be stationary after the maximum uploading time from the initial interval, and thus the transition matrix is invariant of system states of APs.
    % Let $\mat{P}_{m,j}(\Baseline_{t})$ be the transition matrix under baseline policy $\Baseline_{t}$ which is invariant of system states of APs    
\end{lemma}
\begin{proof}
    Please refer to Appendix \ref{append_1}.
\end{proof}

\subsection{The Decentralized Algorithm}
\label{subsec:ap_alg}
Although the optimal value function has been approximated via the baseline policy in the previous part, it is still infeasible for all the APs to solve the RHS with OSI only.
This is because evaluation of equation (\ref{w_ap}) and (\ref{w_es}) requires the knowledge of GSI at each AP.
Instead, it is feasible for part of APs to update their dispatching actions distributively and achieve a better performance compared with baseline policy.
Hence, we first define the following sequence of AP subsets, where APs of each subset are selected to update dispatching actions periodically.
\begin{definition}[Subsets of Periodic Strategy Update]
    Let $\mathcal{Y}_{1}, \dots, \mathcal{Y}_{N} \subseteq \ccSet$ be a sequence of subset, where each subset satisfies the following constraints
    \begin{align}
        &\bigcup_{n=0,\dots,N-1} \mathcal{Y}_{n} = \apSet
        \\
        \esSet_{y} \cap \esSet_{y'} &=\emptyset, y' \neq y~(\forall y',y \in \mathcal{Y}_{n}).
    \end{align}
\end{definition}
For example, as illustrated in Fig.\ref{fig:conflict}, the AP set $\apSet$ could be decomposed of two subsets as $\set{1,3}$ and $\set{2}$.

Hence, in the $t$-th broadcast, the APs in the subset indexed with $n \define t \pmod{N}$ should update their dispatching actions.
Moreover, we shall adopt the baseline policy $\Pi_{y'}$ for the $y'$-th APs ($\forall y' \notin \mathcal{Y}_{n}$).
Let
\begin{align}
    \mathcal{A}(t) \define \set{\mathcal{A}_{y}(t) | \forall y\in\mathcal{Y}_{n}},
\end{align}
where $\mathcal{A}_{y}(t) \define \set{ \tilde{\omega}_{y,j}(t)\in \esSet_{y}|\forall j\in\jSpace }$, 
be the aggregation of dispatching actions for the APs in the subset $\mathcal{Y}_{n}$, and
\begin{align}
    \tilde{\mathcal{A}}(t) \define \mathcal{A}(t) \cup \set{\Pi_{y}(\Stat(t)) | \forall y\notin\mathcal{Y}_{n}}
\end{align}
be the aggregation of dispatching actions of all APs.
The optimization of the dispatching actions $\mathcal{A}_{y}(t)$ ($\forall y\in\mathcal{Y}_{n}$) at the RHS of the Bellman's equations can be rewritten as the following problem.
{\small
\begin{align}
    \textbf{P2:}~
    \min_{ \mathcal{A}_{y}(t) }
    &\sum_{\Stat(t+1)} \Pr\Brace{
        \Stat(t+1) | \Stat(t), \tilde{\mathcal{A}}(t)
    } \cdot W_{\Baseline}\Paren{\Stat(t+1)},
\end{align}
}

Moreover, we have the following conclusion on the decomposition of P2.
\begin{lemma}[]
    The optimization problem in P2 can be equivalently decoupled into the following local optimization problem.
    {\small
    \begin{align}
        \textbf{P3:}~
        \min_{ \mathcal{A}_{y}(t) }
        \mathbb{E}_{\set{ \Stat(t+1), \mathcal{A}_{y}(t) }}
        &\sum_{j\in\jSpace} \sum_{m\in\esSet_{y}} \Brace{
            \tilde{W}^{\AP}_{y,m,j}\Paren{\Stat(t+1)}
            \nonumber\\
            &~~~~~~~~~~~~~~~+\tilde{W}^{\ES}_{m,j}\Paren{\Stat(t+1)}
        }.
        % \nonumber\\
        % &~~~~~~~~~~~\Bracket{
        %     W^{\AP}_{ \Baseline }\Paren{\Stat_{k}(t+1)} +
        %     W^{\ES}_{ \Baseline }\Paren{\Stat_{k}(t+1)}
        % }.
        \label{eqn:partial}
    \end{align}
    }
\end{lemma}
% \begin{proof}
%     We could simply reduce the expression of equation (\ref{w_ap}) and equation (\ref{w_es}) on the RHS of the Bellman's equations based on local OSI, while GSI is not necessary.
%     because the update policy would only change the cost raised on local AP and the corresponding \emph{candidate server set}.
% \end{proof}

The optimization of $\mathcal{A}_{y}(t)$ in P3 could be applied by linear search over the solution space of size $|\esSet_{y}|$.
\begin{remark}[Complexity Analysis of Solving P3]
    The computational complexity of solving P3 with linear search is $O(J(KM+M))$ for evaluating equation (\ref{eqn:partial}).
\end{remark}

The details of the algorithm is elaborated as follows.
\accept{
    \begin{enumerate}
        \item Initialize the all the APs with heuristic policy as dispatching actions $\mathcal{\tilde{A}}(0)$;
        \item At the $1$-st broadcast time slot when $t=0$, the APs in set $\mathcal{Y}_{1}$ and the edge servers in the corresponding \emph{candidate server set}s of APs in $\mathcal{Y}_{1}$, should broadcast their LSI (including the heuristic actions);
        \item Each AP (say the $y_0$-th AP) in set $\mathcal{Y}_{0}$ would receive the OSI at the $D_{y_0}(1)$ time slots in the $1$-st broadcast interval, and then it updates its dispatching actions $\mathcal{A}_{y_0}(1)$ by solving P3;
        % (the APs in set $\mathcal{Y}_{1}$ would keep with the actions until they receive the OSI again);
        % \item At the $2$-nd broadcast time slot when $t=1$, the APs in set $\mathcal{Y}_{2}$ would receive the broadcast OSI;
        \item Similarly, in the $t$-th broadcast interval the APs in subset indexed with $n \define (t+1)\mod{N}$ would receive the OSI, and then update their dispatching actions $\mathcal{A}_{y_n}(t)$ ($\forall y_n\in\mathcal{Y}_{n}$) by solving P3.
        \item Step 4) is periodically repeated with respect to the broadcast interval.
    \end{enumerate}
}
% \begin{remark}[Complexity Analysis]
%     The computation complexity of solving Problem \ref{problem_3} is $O(J())$

%     % The computation complexity of solving Problem \ref{problem_3} is $O(J(c_k M_k+M_k))$ where $c_k, M_k$ is the number of APs in the \emph{conflict AP set} and number of edge servers in the \emph{potential server set} of the $k$-th AP, respectively.
%     Overall, the computation complexity of the algorithm in one period is $O(J(KM+M))$.
% \end{remark}

Finally, we have the following conclusion on the performance of the above proposed algorithm.
\begin{lemma}[Performance Guarantee]
    % The optimized policy solved as $\Baseline(t)$ for next stage, is better than $\Baseline(t-1), \dots, \Baseline(1)$ when considering the \emph{approximated value function} defined above.
    Let $W_{\tilde{\Policy}}(\cdot)$ be the value function of the policy $\tilde{\Policy}$ where
    $\tilde{\Policy}(\Stat(t)) \define \tilde{\mathcal{A}}(t)$,
    \begin{align}
        W_{\tilde{\Policy}}(\Stat) \define
        \sum_{t=1}^{\infty} \gamma^{t-1} \mathbb{E}^{\tilde{\Policy}} \Bracket{
            g\Paren{\Stat(t), \tilde{\Policy}(\Stat(t))} | \Stat(1)=\Stat
        }.
    \end{align}
    The performance of the baseline policies is upper bounded by the optimal solution $\Policy^*$ when considering the original Bellman's equations.
    \begin{align}
        V_{\Policy^*}\Paren{\Stat(t)}
        \leq W_{\tilde{\Policy}}\Paren{\Stat(t)}
        \leq W_{\Baseline}\Paren{\Stat(t)},
        \forall \Stat(t).
    \end{align}
\end{lemma}
\begin{proof}
    
\end{proof}

%----------------------------------------------------------------------------------------%
\delete{v14}{
    Moreover, we notice that the transition function in equation (\ref{eqn:sp_0}) could be rewrite in the following form.
    \begin{align}
        & \Pr\Brace{ \Stat_{k}(t+1)|\Stat_{k}(t), \Omega_{k}(\Stat_{k}(t)) }
        \nonumber\\
        =& \prod_{j\in\jSpace} \Brace{
            \prod_{k'\in\ccSet_{k}}\prod_{m\in\esSet_{k}} \Pr\big\{
                \vec{R}^{(k')}_{m,j}(t+1) | \vec{R}^{(k')}_{m,j}(t), \Omega_{k}(\Stat_{k}(t))
            \big\}
            \nonumber\\
            &\times \prod_{m\in\esSet_{k}} \Pr\big\{
                Q_{m,j}(t+1) | Q_{m,j}(t), \Omega_{k}(\Stat_{k}(t))
            \big\}
        },
    \end{align}
    where the transition function is decoupled into two parts of state transition on APs and edge servers, respectively.

    However, since the GSI and the global \brlatency~ is not obtainable in the edge computing system, we need to further decouple Problem \ref{problem_1} onto individual APs where only local OSI and \brlatency~is required to update the dispatching policy.
    Hence, in the following subsection \ref{subsec:ap_alg}, we propose a decentralized algorithm where the APs shall update their policies simultaneously in the form of disjoint clusters.
    % The performance guarantee of the algorithm is also provided.
}
\delete{v11}{
    % [\IF, \ENDIF], [\FOR, \TO, \ENDFOR], [\WHILE, \ENDWHILE], \STATE, \AND, \TRUE
    % \begin{algorithm}[H]
    %     \caption{Online Iterative Policy Improvement Algorithm}
    %     \begin{algorithmic}[1]
    %         \STATE $t = 0$
    %         \FOR{$t = 1,2,\dots$}
    %             \STATE Evaluate $\Omega_0$ in \textbf{P1} according to equation (\ref{sp1})
    %             \FOR{$k \in \mathcal{K}$}
    %                 \STATE fix policy $\vec{\Omega}^{(k)}(t) \forall k' < k$
    %                 \STATE Evaluate $k$-th AP Local Policy $\tilde{\Omega}_k$ in \textbf{Pk} according to equation (\ref{sp2})
    %             \ENDFOR
    %         \ENDFOR
    %     \end{algorithmic}
    % \end{algorithm}
}
%----------------------------------------------------------------------------------------%