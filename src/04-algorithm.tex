\section{LOW-COMPLEXITY SOLUTION}
\fixit{
    The delay-aware algorithm proposed allow each AP update their policy iteratively, while they all solve the same MDP problem under different control policy.
    This algorithm would lead to performance improvement guarantee with global optimality as upper bound.
}
In this section, we introduce a heuristic dispatching algorithm as the baseline policy, whose value function could be derived analytically.
Then the right-hand side of Bellman's Equation turns into simple minimization problem over the control policy. The sub-optimal policy could be obtained by evaluation of approximated value function on right-hand side of Bellman's Equation with one-step iteration. And thus the approximated value function is the upper bound of the original optimal policy.

\subsection{Baseline Dispatching Policy}
The baseline dispatching policy is adopted to obtain an approximation of value function. The policy on each AP nodes is state-invariant, which is denoted as:
\begin{align}
    \BaseLine(t_i) \define \Bracket{\Pi_1(t_i), \Pi_2(t_i), \dots, \Pi_K(t_i)},
\end{align}
where $\Pi_k(t_i) \define \set{\pi^{(k)}_{m,j}(t_{i}) | \forall m\in\esSet,\forall j\in\jSpace}$ denotes ($\forall k\in\apSet$) the action set resembling the definition in \ref{def_action}.

According to the additive structure of cost function, we substitute the transition function plus value function in Eqn. \ref{sp_0} with two linearly divided sections as $W^{(AP)}(\mathcal{R}(t_{i+1}))$ and $W^{(ES)}(\mathcal{Q}(t_{i+1}))$ under baseline policy $\BaseLine(t_{i})$:
\begin{align}
    &V(\Stat(t_{i})) = g(\Stat(t_{i})) +
    \nonumber\\
    &~~~~~~\gamma \min_{\BaseLine(t_i)} \Bracket{ W^{(AP)}_{\BaseLine(t_i)}\Paren{\mathcal{R}(t_{i+1})} + W^{(ES)}_{\BaseLine(t_i)}\Paren{\mathcal{Q}(t_{i+1})} },
\end{align}
where $W^{(AP)}_{\BaseLine(t_i)}(\cdot)$ and $W^{(ES)}_{\BaseLine(t_i)}(\cdot)$ denote the split \emph{expected value function} over AP nodes and ES nodes respectively, $\Policy(\Stat(t_{i})) = \tilde{\Omega}(t_{i-1}), \BaseLine(t_i)$. The split expected value functions are defined as follows.
\begin{align}
    W^{(AP)}_{\BaseLine(t_i)}(\mathcal{R}(t_{i})) &\define \sum_{k\in\apSet}\sum_{m\in\esSet}\sum_{j\in\jSpace}
        \nonumber\\
        &~~~~\mathbb{E}_{\{\BaseLine(t_i), \vec{r}^{(k)}_{m,j}(t_{i})\}}\Bracket{
            \sum_{h=0}^{\infty} \gamma^{h} \Inorm{\vec{r}^{(k)}_{m,j}(t_{i+h})}
        }
    \\
    W^{(ES)}_{\BaseLine(t_i)}(\mathcal{Q}(t_{i})) &\define \sum_{m\in\esSet}\sum_{j\in\jSpace}
        \nonumber\\
        &~~~~\mathbb{E}_{\{\BaseLine(t_i), Q_{m,j}(t_{i})\}}\Bracket{
            \sum_{h=0}^{\infty} \gamma^{h} l_{m,j}(t_{i+h})
        }.
\end{align}
        
The decoupled value functions for AP and ES nodes is obtained with an approximated form under the baseline policy. We use the same baseline policy to evaluate both low-complexity policy performance in Eqn. \ref{sp_k}.
        
The expected value function $W^{(AP)}(\mathcal{R}(t_{i+1}))$ under baseline $\vecG{\Pi}(t_{i})$ for AP nodes is easily obtained by calculating by the following equation:
\begin{align}
    &W^{(AP)}_{\BaseLine(t_i)}\Paren{\mathcal{R}(t_{i+1})} = \sum_{k\in\apSet}\sum_{m\in\esSet}\sum_{j\in\jSpace}
    \nonumber\\
    & \Inorm{
        \mathbb{E}\Bracket{
            \Paren{
                1 - \gamma \hat{\Gamma}^{(k)}_{m,j}(\BaseLine(t_{i}))
            }^{-1} \times \vecG{\Theta}^{(k)}_{m,j}(t_{i+1})
        }
    },
    \label{w_ap}
\end{align}
where $\Inorm{\cdot}$ denotes the sum of absolute value of each entry of the vector, and $\mathbb[\cdot]$ over the vector of distributions gives out the vector of corresponding expectation. And the transition function $\Pr\{ \mathcal{R}(t_{i+1})|\mathcal{R}(t_{i}) \}$ is embedded with its distributions $\vecG{\Theta}^{(k)}_{m,j}(t_{i+1})$ in the equation.

The expected value function $W^{(ES)}(\mathcal{Q}(t_{i+1}))$ under baseline $\vecG{\Pi}(t_{i})$ for ES nodes is little complex compared to Eqn. \ref{w_ap}.
The ES node transition is affected with both arrival process under dispatching policy and last queue state, and we reduce the states expression by taking average over the uploading process together with the transition function expression.
\begin{align}
    &W^{(ES)}_{\BaseLine(t_i)}\Paren{\mathcal{Q}(t_{i+1})} %\define \mathbb{E}_{\Pi}[\sum_{h=0}^{\infty} \gamma^{h} l_{m,j}(t_{i+h+1})]
    \nonumber\\
    =& \sum_{m,j}\sum_{h=0,\dots,\frac{\Xi}{T}} \gamma^{h} \mathbb{E}[ l_{m,j}(t_{i+h+1}) ]
    \nonumber\\
    &~~~~~~~~+ \gamma^{\frac{\Xi}{T}} \Paren{ \mat{I} - \gamma \hat{\mat{P}}_{m,j}(\tilde{\vecG{\beta}}) }^{-1} \vecG{\nu}(t_{i+\frac{\Xi}{T}+1}) \vec{g}',
\end{align}
where $\hat{\mat{P}}_{m,j}(\tilde{\vecG{\beta}}) \define \prod_{n=0,\dots,N-1} \mat{P}_{m,j}(\tilde{\vecG{\beta}})$;
$l_{m,j}(t_{i+h+1}) \sim \vecG{\nu}(t_{i+h+1})$, and $\vec{\nu}_{m,j}(t_{i+h+1})$ ($\forall h=0,\dots,]\frac{\Xi}{T}$) is obtained by calculation over equation (\ref{eqn_0})-(\ref{eqn_4});
the $i$-th element of vector $\vec{g}$ denotes the cost of server as $l_{m,j}(t_i)$;
$\tilde{\vecG{\beta}}$ is the arrival distribution under baseline policy $\Pi(t_{i})$ (on $m$-th ES with $j$-type job), where:
\begin{align}
    ~~~~\tilde{\vecG{\beta}} &\define [\tilde{\beta}^{(1)}_{m,j}, \tilde{\beta}^{(0)}_{m,j}]
    \\
    ~~~~\tilde{\beta}^{(1)}_{m,j} &\define \sum_{k\in\apSet} \lambda^{(k)}_{m,j} \times \sum_{\xi=0,\dots,\Xi-1} \Pr\{ \xi<U_{k,m}\leq(\xi+1) \}
        \nonumber\\
    ~~~~&= \sum_{k\in\apSet} \lambda^{(k)}_{m,j}
    \\
    ~~~~\tilde{\beta}^{(0)}_{m,j} &= 1 - \tilde{\beta}^{(1)}_{m,j}
\end{align}
%----------------------------------------------------------------------------------------%

\subsection{The Distributed Algorithm}
The optimization problem at right-hand side of approximate Bellman's Equation is given as follows.
\begin{align}
    \min_{\BaseLine(t_i)} W^{(AP)}_{\BaseLine(t_i)}\Paren{\mathcal{R}(t_{i+1})} + W^{(ES)}_{\BaseLine(t_i)}\Paren{\mathcal{Q}(t_{i+1})}
    \label{eqn_opt}
\end{align}

Then we introduce the iterative policy update algorithm, which optimize the 
with improved baseline policy in each interval as performance guarantee.
\begin{itemize}
    \item Choose initial policy $\Policy(\Stat(t_0)) = \BaseLine(t_0)$ as start, which is the \emph{local greedy policy};
    \item In $[t_0, t_1]$, the AP node indexed with $1$ would receive the broadcast information after $D_1(t_1)$ and updates the global information; then it solves the optimization problem in \ref{eqn_opt} with only $\Pi_{1}(t_1)$ changes in $\Policy(\Stat(t_1)) = \BaseLine(t_1) = [\Pi_{1}(t_1), \Pi_{2}(t_0), \dots, \Pi_{K}(t_0)]$;
    \item At $t_1$, the AP node indexed with $1$ would broadcast its policy $\Pi_{1}(t_1)$ together with the state information;
    \item In $[t_1, t_2]$, the AP nodes indexed with $2$ would receive the broadcast information and previous global policy $\Policy(\Stat(t_1))$; then it repeats the procedure what the $1$-st AP did;
    \item Then generally, in $[t_{i}, t_{i+1}]$, the AP nodes indexed with $(k \mod K)+1$ would update its own policy following the above procedure.
\end{itemize}

% [\IF, \ENDIF], [\FOR, \TO, \ENDFOR], [\WHILE, \ENDWHILE], \STATE, \AND, \TRUE
% \begin{algorithm}[H]
%     \caption{Online Iterative Policy Improvement Algorithm}
%     \begin{algorithmic}[1]
%         \STATE $t = 0$
%         \FOR{$t = 1,2,\dots$}
%             \STATE Evaluate $\Omega_0$ in \textbf{P1} according to Eqn. \ref{sp1}
%             \FOR{$k \in \mathcal{K}$}
%                 \STATE fix policy $\vec{\Omega}^{(k)}(t) \forall k' < k$
%                 \STATE Evaluate $k$-th AP Local Policy $\tilde{\Omega}_k$ in \textbf{Pk} according to Eqn. \ref{sp2}
%             \ENDFOR
%         \ENDFOR
%     \end{algorithmic}
% \end{algorithm}

\begin{lemma}
    The above algorithm would have performance improvement in each interval with baseline policy guarantee, and at least achieve local optimal.
\end{lemma}
\begin{proof}
    The improved policy in last interval becomes the baseline policy in next interval.
    So, in each stage, it resembles the one-step Policy Improvement step in classical \emph{Policy Iteration} algorithm, and each step with different value function under baseline policy on the right-hand side.
    
    \fixit{The insight exists that: due to the uncontrollable delay on policy in each stage,} the chosen time-invariant baseline policy actually would; so the improved policy is the baseline of next stage, and thus forms the online policy iteration.
    discounted sum-up cost, the future states near the current state counts more and the baseline policy (which is time-invariant) would variant in each interval.
\end{proof}

\subsection{Reinforcement Learning}
Due to the fact that the statistical parameters of job arrival distribution $\lambda_{k,j}$, uploading time distribution $U_{k,m,j}$ and processing time distribution $l_{m,j}$ are unknown at the first time, we adopt reinforcement learning algorithm to learn the parameters.
%----------------------------------------------------------------------------------------%