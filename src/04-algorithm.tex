\section{LOW-COMPLEXITY SOLUTION}
In this section, we introduce a heuristic dispatching algorithm as the baseline policy, whose value function could be derived analytically.
Then the right-hand side of Bellman's Equation turns into simple minimization problem over the control policy. The sub-optimal policy could be obtained by evaluation of approximated value function on right-hand side of Bellman's Equation with one-step iteration. And thus the approximated value function is the upper bound of the original optimal policy.

Moreover, as the system state is partially presented as $\Obsv({t-1})$ and $\Obsv(t)$ separated by the \brlatency~$\mathcal{D}(t)$, different APs would have isolated acknowledge on the joint policy and transition function due to different $D_k(t)$ ($\forall k\in\apSet$).
Thus the joint optimization over the components of $\Policy(\Stat(t))$ is impossible.
To overcome the gap, we leverage the above global problem structure, but solve it with iterative policy update algorithm.

\subsection{Baseline Dispatching Policy}
The baseline dispatching policy is adopted to obtain an approximation of value function. The policy on each AP nodes is state-invariant, which is denoted as:
\begin{align}
    \BaseLine(t) \define \Bracket{\Pi_1(t), \Pi_2(t), \dots, \Pi_K(t)},
\end{align}
where $\Pi_k(t) \define \set{\pi^{(k)}_{m,j}(t) | \forall m\in\esSet,\forall j\in\jSpace}$ denotes ($\forall k\in\apSet$) the action set resembling the definition in Eqn. (\ref{def_action}).
More specifically, $\BaseLine(t)$ is assumed as the stationary baseline policy adopted globally after the $t$-th \brpoint~and the baseline policies are different in each broadcast interval.

According to the additive structure of cost function, we substitute the transition function plus value function in Eqn. (\ref{sp_0}) with two linearly divided sections as $W^{\AP}(\mathcal{R}(t+1))$ and $W^{\ES}(\mathcal{Q}(t+1))$ under baseline policy $\BaseLine(t)$:
\begin{align}
    &V(\Stat(t)) = g(\Stat(t)) +
    \nonumber\\
    &~~~~~~\gamma \min_{\BaseLine(t)} \Bracket{ W^{\AP}_{\BaseLine(t)}\Paren{\mathcal{R}(t+1)} + W^{\ES}_{\BaseLine({i})}\Paren{\mathcal{Q}(t+1)} },
\end{align}
where $W^{\AP}_{\BaseLine(t)}(\cdot)$ and $W^{\ES}_{\BaseLine(t)}(\cdot)$ denote the split \emph{expected value function} over AP nodes and ES nodes, respectively, $\Policy(\Stat(t)) = \tilde{\Omega}({t-1}), \BaseLine(t)$. The split expected value functions are defined as follows.
\begin{align}
    W^{\AP}_{\BaseLine(t)}\Paren{\mathcal{R}(t)}
        &\define \sum_{k\in\apSet}\sum_{m\in\esSet}\sum_{j\in\jSpace}
        % \nonumber\\
        \mathbb{E}_{\{\BaseLine(t), \vec{R}^{(k)}_{m,j}(t)\}}\Bracket{
            \sum_{i=0}^{\infty} \gamma^{i} \Inorm{\vec{R}^{(k)}_{m,j}({t+i})}
        }
    \\
    W^{\ES}_{\BaseLine(t)}\Paren{\mathcal{Q}(t)}
        &\define \sum_{m\in\esSet}\sum_{j\in\jSpace}
        % \nonumber\\
        \mathbb{E}_{\{\BaseLine(t), Q_{m,j}(t)\}}\Bracket{
            \sum_{i=0}^{\infty} \gamma^{i} l_{m,j}({t+i})
        }.
\end{align}
        
The decoupled value functions for AP and ES nodes is obtained with an approximated form under the baseline policy. We use the same baseline policy to evaluate both low-complexity policy performance in Eqn. (\ref{sp_0}).

The expected value function $W^{\AP}(\mathcal{R}(t+1))$ under baseline $\vecG{\Pi}(t)$ for AP nodes is easily obtained by calculating by the following equation:
\begin{align}
    &W^{\AP}_{\BaseLine(t)}\Paren{\mathcal{R}(t+1)} = \sum_{k\in\apSet}\sum_{m\in\esSet}\sum_{j\in\jSpace}
    % \nonumber\\
    \Inorm{
        \Paren{1 - \gamma \hat{\Gamma}^{(k)}_{m,j}}^{-1}
        \times \vecG{\Theta}^{(k)}_{m,j}(t+1)
    },
    \label{w_ap}
\end{align}
where $\hat{\Gamma}^{(k)}_{m,j} \define (\Gamma^{(k)}_{m,j})^{N}$, $\Inorm{\cdot}$ denotes the sum of absolute value of each entry of the vector.
\delete{v4}{And the transition function $\Pr\{ \mathcal{R}(t+1)|\mathcal{R}(t) \}$ is embedded with its distributions $\vecG{\Theta}^{(k)}_{m,j}(t+1)$ in the equation.}

The expected value function $W^{\ES}(\mathcal{Q}(t+1))$ under baseline $\vecG{\Pi}(t)$ for ES nodes is little more complex compared to Eqn. (\ref{w_ap}).
The ES node transition is affected with both arrival process under dispatching policy and last queue state, and we reduce the states expression by taking average over the uploading process together with the transition function expression.
\begin{align}
    &W^{\ES}_{\BaseLine(t)}\Paren{\mathcal{Q}(t+1)}
    \nonumber\\
    =& \sum_{m,j}\sum_{i=0,\dots,\frac{\Xi}{T}} \gamma^{i} \mathbb{E}[ l_{m,j}({t+i+1}) ]
    \nonumber\\
    &~~~~~~~~+ \gamma^{\frac{\Xi}{T}} \Paren{ \mat{I} - \gamma \hat{\mat{P}}_{m,j}(\tilde{\beta}_{m,j}) }^{-1} \vecG{\nu}({t+\frac{\Xi}{T}+1}) \vec{g}',
\end{align}
where $\hat{\mat{P}}_{m,j}(\tilde{\beta}_{m,j}) \define \prod_{n=0,\dots,N-1} \mat{P}_{m,j}(\tilde{\beta}_{m,j})$;
the probability distribution of $l_{m,j}({t+i+1})$ is denoted by $\vecG{\nu}({t+i+1})$ which is obtained by calculation over equation Enq. (\ref{eqn_0}) - Eqn. (\ref{eqn_4}) ($\forall i=0,\dots,\frac{\Xi}{T}$);
the $i$-th element of vector $\vec{g}$ denotes the cost of server as $l_{m,j}(t)$;
$\tilde{\beta}_{m,j}$ is the arrival distribution under baseline policy $\Pi(t)$ (on $m$-th ES with $j$-type job), where:
\begin{align}
    \tilde{\beta}_{m,j} &\define \sum_{k\in\apSet} \tilde{\lambda}^{(k)}_{m,j} \times \sum_{\xi=0,\dots,\Xi-1} \Pr\{ \xi<U_{k,m}\leq(\xi+1) \}
        \nonumber\\
    ~~~~&= \sum_{k\in\apSet} \tilde{\lambda}^{(k)}_{m,j}
\end{align}
%----------------------------------------------------------------------------------------%

\subsection{Problem Decoupling with Partial Observed Information}
It's still impossible to evaluate the above Bellman's equation due to the complexity would increase exponentially with respect to number of APs and edge servers.
Thus we leverage the fact that:
%NOTE: Conflict of AP set and partial information definition
In \comments{an extensive edge computing network residing in MAN}, it may not be feasible for each AP to collect the LSI from all other APs and edge servers.
Hence, we first define the \emph{conflict AP set} as follows.
\begin{align}
    \ccSet_{k} \define \bigcup_{m\in\esSet_{k}} \apSet_{m}
    % \ccSet_{k} \define \set{\forall k' \neq k\in\apSet|\esSet_{k'} \cap \esSet_{k} \neq \emptyset}
\end{align}
The \emph{conflict AP set} indicates that the subset of APs whose LSI could affect the decision making for the $k$-th AP.
It is assumed that each AP can only collect the LSI from the APs in its \emph{conflict AP set} and edge servers from its \emph{candidate server set}.
\begin{definition}[Observed State Information]
    Hence, we can define the \emph{observed state information} (OSI) of the $k$-th AP as follows.
    \begin{align}
        \Obsv_{k} &= \set{\mathcal{R}_{k'} | \forall k'\in\ccSet_{k}}
                        \cup \set{\mathcal{Q}_{m} | \forall m\in\esSet_{k}},
    \end{align}
\end{definition}

\fixit{
    One AP will update its dispatching decision after the reception of its OSI.
    We denote the individual dispatching policy of the $k$-th AP ($\forall k\in\apSet$) based on its OSI $\Obsv_{k}({t})$ as follows.
    \begin{align}
        &\Omega_{k}(\Obsv_{k}(t)) \define \set{\omega'_{k,j}(t)|\forall m\in\esSet, j\in\jSpace}.
        \label{def_action}
    \end{align}

    Hence, we only update the individual policy of the $k$-th AP ($\forall k\in\apSet$) at one time, and the Bellman's equation would reduce into the form as follows, where only the OSI (partial information) for the $k$-th AP matters.
    \begin{align}
        V_{k}\Paren{\Obsv_{k}({t})} =& g_{k}(\Obsv_{k}(t)) +
        \gamma\min_{\Omega_{k}(\Obsv_{k}(t))}
        \nonumber\\
        &\sum_{\Obsv_{k}(t+1)} \Pr
        \Brace{ 
            \Obsv_{k}(t+1) | \Obsv_{k}(t), \Policy_{k}(t-1), \Omega_{k}(\Obsv_{k}(t))
        } \cdot V_{k}\Paren{\Obsv_{k}({t+1})},
    \end{align}
    where,
    \begin{align}
        g_{k}(\Obsv_{k}(t)) \define& \sum_{j\in\jSpace} \Brace{
            \sum_{k\in\mathcal{X}_{k}} \sum_{m\in\esSet_{k}} \vec{R}^{(k)}_{m,j}(t) +
            \sum_{m\in\esSet_{k}} \set{ L_{m,j}(t) + \beta \cdot \mat{I}[L_{m,j}(t)=L_{max}] }
        }
        \\
        \Policy_{k}(t-1) \define & \Brace{ \Omega_{k'}(\Obsv_{k'}(t-1)) | \forall k'\in\mathcal{X}_{k} }
    \end{align}
}
%----------------------------------------------------------------------------------------%

\subsection{The Distributed Algorithm}
\fixit{
    \begin{itemize}
        \item We leverage the global optimization problem, while the presence of $\mathcal{D}(t)$ doesn't matter in our algorithm;
        \item We propose one iterative update algorithm (elaboration needed);
        \item The performance bound is guaranteed, and always better compared with last stage.
    \end{itemize}
}

The optimization problem at right-hand side of approximate Bellman's Equation is given as follows.
\begin{align}
    \min_{\BaseLine(t)} W^{\AP}_{\BaseLine(t)}\Paren{\mathcal{R}(t+1)} + W^{\ES}_{\BaseLine(t)}\Paren{\mathcal{Q}(t+1)}
    \label{eqn_opt}
\end{align}

Then we introduce the iterative policy update algorithm, which optimize the 
with improved baseline policy in each interval as performance guarantee.
\begin{itemize}
    \item Choose initial policy $\Policy(\Stat(0)) = \BaseLine(0)$ as start, which is the \emph{local greedy policy};
    \item In $t\in[0, 1)$, the AP node indexed with $1$ would receive the broadcast information after $D_1(1)$ and updates the global information; then it solves the optimization problem in Eqn. (\ref{eqn_opt}) with only $\Pi_{1}(1)$ changes in $\Policy(\Stat(1)) = \BaseLine(1) = [\Pi_{1}(1), \Pi_{2}(0), \dots, \Pi_{K}(0)]$;
    \item At $t=1$, the AP node indexed with $1$ would broadcast its policy $\Pi_{1}(1)$ together with the state information;
    \item In $t\in[1, 2)$, the AP nodes indexed with $2$ would receive the broadcast information and previous global policy $\Policy(\Stat(1))$; then it repeats the procedure what the $1$-st AP did;
    \item Then generally, in $t\in[{i}, {i+1})$, the AP nodes indexed with $(k \mod K)+1$ would update its own policy following the above procedure.
\end{itemize}

\begin{lemma}
    The optimized policy solved as $\BaseLine(t)$ (for next stage), is better than $\BaseLine(t-1), \dots, \BaseLine(1)$ when considering the \emph{approximated value function} defined above.
    \\
    Moreover, the performance of the series of the baseline policies is upper bounded by the optimal solution $\Policy^*$ when considering the original Bellman's equation.
\end{lemma}
\begin{proof}
    \fixit{
        The proof is illustrated into two parts.
        \begin{itemize}
            \item $\BaseLine(t)$ is better than the previous baseline policy, \emph{with the latest start} $\Obsv(t+1)$; the other previous policies were determined from the different start state;
            \item Upper bound is easy to prove with one-step iteration that:
            $$
                V_{\Omega^*}\Paren{\Stat(t)}
                    \leq W_{\BaseLine(t)}\Paren{\Stat(t)}
                    \leq W_{\BaseLine(t-1)}\Paren{\Stat(t)}
                    \leq \dots \leq W_{\Pi(1)}\Paren{\Stat(t)}
            $$
            but I doube this could not stand, cause the stationary policy is also affected by \emph{start state} and \emph{random broadcat delay}.
        \end{itemize}
    }
\end{proof}
% [\IF, \ENDIF], [\FOR, \TO, \ENDFOR], [\WHILE, \ENDWHILE], \STATE, \AND, \TRUE
% \begin{algorithm}[H]
%     \caption{Online Iterative Policy Improvement Algorithm}
%     \begin{algorithmic}[1]
%         \STATE $t = 0$
%         \FOR{$t = 1,2,\dots$}
%             \STATE Evaluate $\Omega_0$ in \textbf{P1} according to Eqn. \ref{sp1}
%             \FOR{$k \in \mathcal{K}$}
%                 \STATE fix policy $\vec{\Omega}^{(k)}(t) \forall k' < k$
%                 \STATE Evaluate $k$-th AP Local Policy $\tilde{\Omega}_k$ in \textbf{Pk} according to Eqn. \ref{sp2}
%             \ENDFOR
%         \ENDFOR
%     \end{algorithmic}
% \end{algorithm}

\delete{v4}{
    \begin{lemma}
        The above algorithm would have performance improvement in each interval with baseline policy guarantee, and at least achieve local optimal.
    \end{lemma}
    \begin{proof}
        The improved policy in last interval becomes the baseline policy in next interval.
        So, in each stage, it resembles the one-step Policy Improvement step in classical \emph{Policy Iteration} algorithm, and each step with different value function under baseline policy on the right-hand side.
        
        The insight exists that: due to the uncontrollable delay on policy in each stage, the chosen time-invariant baseline policy actually would; so the improved policy is the baseline of next stage, and thus forms the online policy iteration.
        discounted sum-up cost, the future states near the current state counts more and the baseline policy (which is time-invariant) would variant in each interval.
    \end{proof}
}
%----------------------------------------------------------------------------------------%