\section{Decentralized Algorithm with Partial Information}
\label{sec:algorithm}
Generally speaking, the optimal policy could be obtained by solving the minimization problem on the right-hand-side of the above Bellman's equation. %Eqn. (\ref{eqn:sp_0}).
In our problem, however, the GSI is not available and the centralized agent design is impractical due to randomness of \brlatency.
On the other side, conventional value iteration algorithm is intractable due to the tremendous state space.
The number of system states and action space would grow exponentially with respect to the number of APs and edge servers.
In this section, we propose a low-complexity solution scheme where each AP updates its dispatching policy according to its OSI in an iterative manner.

\subsection{Low-Complexity Solution Framework}
In this part, we first introduce an approximation of value function.
Based on it, the optimization of the right-hand-side of the Bellman's equations in Eqn.(\ref{eqn:sp_0}) can be decoupled.
Specifically, the value function of the optimal policy is approximated as follows.
\begin{align}
    V\Paren{\Stat(t)} \approx&
        W^{\AP}_{\Baseline}\Paren{\mathcal{R}(t+1)} + W^{\ES}_{\Baseline}\Paren{\mathcal{Q}(t+1)}
    \nonumber\\
    =& \sum_{j\in\jSpace}\sum_{k\in\apSet}\sum_{m\in\esSet} \Bracket{
        \sum_{i=0}^{\infty} \gamma^{i+1} \Inorm{ \vecG{R}^{(k)}_{m,j}(t+i+1) }
    }
    \nonumber\\
    & + \sum_{j\in\jSpace}\sum_{m\in\esSet} \Bracket{
        \sum_{i=0}^{\infty} \gamma^{i+1} Q_{m,j}(t+i+1)
    },
    \label{eqn:approx}
\end{align}
where $\Baseline$ denotes the following baseline policy.
\begin{definition}[Baseline Policy]
    \begin{align}
        \Baseline &\define \Bracket{ \Pi_{1}, \dots, \Pi_{K} }
        \\
        \Pi_{k} &\define \Brace{\pi_{k,j}\in\esSet|\forall j\in\jSpace}, \forall k\in\apSet
    \end{align}
    The baseline policy is chosen as fixed policy which is invariant of system state.
\end{definition}

However, with the approximate value function, it is still infeasible to solve the right-hand-side of the Bellman's equations.
On the one hand, the GSI is not available and each AP updates its policy independently from its corresponding OSI.
On the other hand, the centralized agent design is impractical due to randomness of \brlatency.

Hence, we propose a novel alternative policy update framework to address the issue of incomplete system state information.
Specifically, we first introduce the following periodic AP update sequence.
The APs shall update their polices iteratively based on OSI following the order of index of set $apSet$ and then repeat the procedure with the period $K$.
Based on it, we propose the following \emph{alternative dispatching policy}.
\begin{definition}[Alternative Dispatching Policy]
    \begin{align}
        \tilde{\Policy}\Paren{ k, \Stat(t), \Delay(t) } \define \Brace{
            \Omega_{k}( \Stat_{k}(t), \mathcal{D}_{k}(t) )
        },
    \end{align}
    where $k+1 \equiv t \pmod{K}, \forall t$.
\end{definition}
\begin{remark}
    In the \emph{alternative dispatching policy}, the index $k$ indicates that in the $t$-th interval only the $k$-th AP would update its policy while the other APs with policy fixed as broadcast in OSI of the $k$-th AP.
\end{remark}

Based on the proposed policy update framework, we further formulate the optimization problem as follows.
\begin{problem}[Alternative Optimization Problem]
    \begin{align}
        \min_{\tilde{\Policy}} \lim_{T\to\infty} \mathbb{E}_{\tilde{\Policy}} \Bracket{
            \sum_{t=1}^{T} \gamma^{(t-1)} g\Paren{
                \Stat(t), \tilde{\Policy}(\Stat(t), \Delay(t))
            } | \Stat(1)
        }.
    \end{align}
    \label{problem_2}
\end{problem}

\begin{lemma}
    With the \emph{alternative dispatching policy} framework, the approximated value function Eqn.(\ref{eqn:approx}) can be decoupled onto each AP in the following form.
    \begin{align}
        V_{k}(\Stat_{k}(t)) &\define W^{\AP}_{\Pi_{k}(t)}\Paren{\mathcal{R}(t+1)} + W^{\ES}_{\Pi_{k}(t)}\Paren{\mathcal{Q}(t+1)}
        \nonumber\\
        =& \sum_{j\in\jSpace}\sum_{k\in\ccSet_{k}}\sum_{m\in\esSet_{k}} \Bracket{
            \sum_{i=0}^{\infty} \gamma^{i+1} \Inorm{ \vecG{R}^{(k')}_{m,j}(t+i+1) }
        }
        \nonumber\\
        & + \sum_{j\in\jSpace}\sum_{m\in\esSet_{k}} \Bracket{
            \sum_{i=0}^{\infty} \gamma^{i+1} Q_{m,j}(t+i+1)
        },
    \end{align}
\end{lemma}
where
\begin{align}
    \Pi_{k}(t) = \set{\omega_{k,j}(t-1)|\forall j\in\jSpace}.
\end{align}

\subsection{The Decentralized Algorithm}
Then, we could propose the algorithm to solve the decentralized problem for each AP.
The decentralized problem for the AP is defined as follows.
% The optimization problem at right-hand side of approximate Bellman's Equation is given as follows.
\begin{problem}[Decentralized Approximated Optimization Problem for AP]
    For the $k$-th AP with baseline policy $\Pi_{k}(t)$:
    \begin{align}
        \min_{\Pi_{k}(t)} \mathbb{E}_{\set{ \Stat(t+1), \Pi_{k}(t) }} \Bracket{
            W^{\AP}_{\Pi_{k}(t)}\Paren{\mathcal{R}(t+1)} + W^{\ES}_{\Pi_{k}(t)}\Paren{\mathcal{Q}(t+1)}
        }.
        \label{eqn:partial}
    \end{align}
\end{problem}

The expected value function $W^{\AP}_{\Pi_{k}(t)}(\mathcal{R}(t+1))$ is easily obtained by calculating the following equation.
\begin{align}
    W^{\AP}_{\Pi_{k}(t)}\Paren{\mathcal{R}(t+1)} = \sum_{j\in\jSpace}&\sum_{k\in\ccSet_{k}}\sum_{m\in\esSet_{k}}
    \nonumber\\
    &\Inorm{
        \Paren{ 1 - \gamma ({\Gamma}^{(k)}_{m,j})^{N} }^{-1} \hat{\vecG{\Theta}}^{(k)}_{m,j}(t+1)
    }.
    \label{w_ap}
\end{align}
% where $\hat{\Gamma}^{(k)}_{m,j} \define (\Gamma^{(k)}_{m,j})^{N}$.

The expression for expected value function $W^{\ES}_{\Pi_{k}(t)}(\mathcal{Q}(t+1))$ is little more complex compared to Eqn.(\ref{w_ap}).
It is affected with both arrival process under dispatching policy and last queue state.
However, we notice that the arrival process would be stationary after the maximum uploading time under the stationary baseline policy and the relationship between APs and edge server could be decoupled.
\begin{align}
    W^{(k)}_{\Baseline(t)}\Paren{\mathcal{Q}(t+1)}
    =& \sum_{j\in\jSpace}\sum_{m\in\esSet_{k}}\sum_{i=0,\dots,\frac{\Xi}{T}} \gamma^{i} \mathbb{E}[ Q_{m,j}({t+i+1}) ]
    \nonumber\\
    &~~~~~~~~+ \gamma^{\frac{\Xi}{T}} \Paren{ \mat{I} - \gamma \hat{\mat{P}}_{m,j}(\tilde{\beta}_{m,j}) }^{-1} \vecG{\nu}({t+\frac{\Xi}{T}+1}) \vec{g}',
\end{align}
where the $i$-th element of vector $\vec{g}$ denotes the cost of server as $Q_{m,j}(t)$;
\begin{align}
    \hat{\mat{P}}_{m,j}(\tilde{\beta}_{m,j}) \define \prod_{n=0,\dots,N-1} \mat{P}_{m,j}(\tilde{\beta}_{m,j})
\end{align}
% the probability distribution of $Q_{m,j}({t+i+1})$ is denoted by $\vecG{\nu}({t+i+1})$ which is obtained by calculating equation Enq. (\ref{eqn_0}) - Eqn. (\ref{eqn_4}) ($\forall i=0,\dots,\frac{\Xi}{T}$);
where $\tilde{\beta}_{m,j}$ is the arrival distribution under baseline policy $\Pi(t)$ (on $m$-th ES with type $j$ jobs)
\begin{align}
    \tilde{\beta}_{m,j} &\define \sum_{k\in\apSet} \tilde{\lambda}^{(k)}_{m,j} \times \sum_{\xi=0,\dots,\Xi-1} \Pr\{ \xi<U_{k,m}\leq(\xi+1) \}
        \nonumber\\
    ~~~~&= \sum_{k\in\apSet} \tilde{\lambda}^{(k)}_{m,j}
\end{align}

The details of the procedure is elaborated as follows.
\begin{itemize}
    \item Choose the initial \emph{system dispatching policy} start with some heuristic policy.
    \item At the first broadcast time slot when $t=0$, the APs and edge servers in \emph{conflict AP set} and \emph{candidate server set}, respectively, of the $1$-st AP would broadcast their LSI (including the heuristic policies);
    \item The $1$-st AP would receive the OSI at the $D_1(1)$ time slots in the first broadcast interval, and then it updates its policy $\Omega_{1}(\Stat_{1}(1))$ by solving Eqn.(\ref{eqn:sp_0}) for the $1$-st AP.
    The $1$-st AP would keep with this policy until it receives the OSI again.
    \item At the second broadcast time slot when $t=1$, the OSI of the $2$-nd AP would be broadcast;
    the $1$-st AP would broadcast $\Omega_{1}(\Stat_{1}(1))$ if it's in the \emph{conflict AP set} of the $2$-nd AP;
    \item Similarly, in the $t$-th broadcast interval the AP indexed with $k' \equiv (t + 1)\mod{K}$ would receive the OSI $D_{k'}(t)$ time slots later, and then it updates its policy $\Omega_{k'}(t+1)$ by solving Eqn.(\ref{eqn:sp_0}) for the $k'$-th AP.
\end{itemize}

% \begin{lemma}
%     The optimized policy solved as $\Baseline(t)$ for next stage, is better than $\Baseline(t-1), \dots, \Baseline(1)$ when considering the \emph{approximated value function} defined above.
%     \\
%     Moreover, the performance of the series of the baseline policies is upper bounded by the optimal solution $\Policy^*$ when considering the original Bellman's equation.
%     $$
%         V_{\Omega^*}\Paren{\Stat(t)}
%         \leq W_{\tilde{\Policy}(t)}\Paren{\Stat(t)}
%         \leq W_{\tilde{\Policy}(t-1)}\Paren{\Stat(t)}
%     $$
% \end{lemma}
% \begin{proof}
    
% \end{proof}

%----------------------------------------------------------------------------------------%
\delete{v11}{
    % [\IF, \ENDIF], [\FOR, \TO, \ENDFOR], [\WHILE, \ENDWHILE], \STATE, \AND, \TRUE
    % \begin{algorithm}[H]
    %     \caption{Online Iterative Policy Improvement Algorithm}
    %     \begin{algorithmic}[1]
    %         \STATE $t = 0$
    %         \FOR{$t = 1,2,\dots$}
    %             \STATE Evaluate $\Omega_0$ in \textbf{P1} according to Eqn. (\ref{sp1})
    %             \FOR{$k \in \mathcal{K}$}
    %                 \STATE fix policy $\vec{\Omega}^{(k)}(t) \forall k' < k$
    %                 \STATE Evaluate $k$-th AP Local Policy $\tilde{\Omega}_k$ in \textbf{Pk} according to Eqn. (\ref{sp2})
    %             \ENDFOR
    %         \ENDFOR
    %     \end{algorithmic}
    % \end{algorithm}
}
%----------------------------------------------------------------------------------------%