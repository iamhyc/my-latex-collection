\section{LOW-COMPLEXITY SOLUTION}
In this section, we introduce a heuristic dispatching algorithm as the baseline policy, whose value function could be derived analytically.
Then the right-hand side of Bellman's Equation turns into simple minimization problem over the control policy. The sub-optimal policy could be obtained by evaluation of approximated value function on right-hand side of Bellman's Equation with one-step iteration. And thus the approximated value function is the upper bound of the original optimal policy.

\subsection{Baseline Dispatching Policy}
The baseline dispatching policy is adopted to obtain an approximation of value function. The policy on each AP nodes is state-invariant, which is denoted as:
\begin{align}
    \BaseLine(t_i) \define \Bracket{\Pi_1(t_i), \Pi_2(t_i), \dots, \Pi_K(t_i)},
\end{align}
where $\Pi_k(t_i) \define \set{\pi^{(k)}_{m,j}(t_{i}) | \forall m\in\esSet,\forall j\in\jSpace}$ denotes ($\forall k\in\apSet$) the action set resembling the definition in \ref{def_action}.

According to the additive structure of cost function, we substitute the transition function plus value function in Eqn. \ref{sp_0} with two linearly divided sections as $W^{(AP)}(\mathcal{R}(t_{i+1}))$ and $W^{(ES)}(\mathcal{Q}(t_{i+1}))$ under baseline policy $\BaseLine(t_{i})$:
\begin{align}
    &V(\Stat(t_{i})) = g(\Stat(t_{i})) +
    \nonumber\\
    &~~~~~~\gamma \min_{\BaseLine(t_i)} \Bracket{ W^{(AP)}_{\BaseLine(t_i)}\Paren{\mathcal{R}(t_{i+1})} + W^{(ES)}_{\BaseLine(t_i)}\Paren{\mathcal{Q}(t_{i+1})} },
\end{align}
where $W^{(AP)}_{\BaseLine(t_i)}(\cdot)$ and $W^{(ES)}_{\BaseLine(t_i)}(\cdot)$ denote the split \emph{expected value function} over AP nodes and ES nodes respectively, $\Policy(\Stat(t_{i})) = \tilde{\Omega}(t_{i-1}), \BaseLine(t_i)$. The split expected value functions are defined as follows.
\begin{align}
    W^{(AP)}_{\BaseLine(t_i)}(\mathcal{R}(t_{i})) &\define \sum_{k\in\apSet}\sum_{m\in\esSet}\sum_{j\in\jSpace}
        \nonumber\\
        &~~~~\mathbb{E}_{\{\BaseLine(t_i), \vec{r}^{(k)}_{m,j}(t_{i})\}}\Bracket{
            \sum_{h=0}^{\infty} \gamma^{h} \Inorm{\vec{r}^{(k)}_{m,j}(t_{i+h})}
        }
    \\
    W^{(ES)}_{\BaseLine(t_i)}(\mathcal{Q}(t_{i})) &\define \sum_{m\in\esSet}\sum_{j\in\jSpace}
        \nonumber\\
        &~~~~\mathbb{E}_{\{\BaseLine(t_i), Q_{m,j}(t_{i})\}}\Bracket{
            \sum_{h=0}^{\infty} \gamma^{h} l_{m,j}(t_{i+h})
        }.
\end{align}
        
The decoupled value functions for AP and ES nodes is obtained with an approximated form under the baseline policy. We use the same baseline policy to evaluate both low-complexity policy performance in Eqn. \ref{sp_k}.
        
The expected value function $W^{(AP)}(\mathcal{R}(t_{i+1}))$ under baseline $\vecG{\Pi}(t_{i})$ for AP nodes is easily obtained by calculating by the following equation:
\begin{align}
    &W^{(AP)}_{\BaseLine(t_i)}\Paren{\mathcal{R}(t_{i+1})} = \sum_{k\in\apSet}\sum_{m\in\esSet}\sum_{j\in\jSpace}
    \nonumber\\
    & \Inorm{
        \mathbb{E}\Bracket{
            \Paren{
                1 - \gamma \hat{\Gamma}^{(k)}_{m,j}(\BaseLine(t_{i}))
            }^{-1} \times \vecG{\Theta}^{(k)}_{m,j}(t_{i+1})
        }
    },
    \label{w_ap}
\end{align}
where $\Inorm{\cdot}$ denotes the sum of absolute value of each entry of the vector, and $\mathbb[\cdot]$ over the vector of distributions gives out the vector of corresponding expectation. And the transition function $\Pr\{ \mathcal{R}(t_{i+1})|\mathcal{R}(t_{i}) \}$ is embedded with its distributions $\vecG{\Theta}^{(k)}_{m,j}(t_{i+1})$ in the equation.

% NOTE: [Approximation with average on simple expectation]
% The approximated value function $\tilde{W}^{(s)}(\Obsv(i))$ for ES nodes are different for two equations for different transition function expression.
% We firstly express the approximated value function in Eqn. \ref{sp2} and then express the one in Eqn. \ref{sp1} with respect to it.
% The ES node transition is affected with both arrival process under dispatching policy and last queue state, and we reduce the states expression by taking average over the uploading process together with the transition function expression.
% The partial value function optimization for ES in Bellman's Equation right-hand side in Eqn. \ref{sp2} is given as follows.
% \begin{align}
%     \min_{\Policy(\Stat_i)}& \sum_{\Stat_{i+1}}
%         \Pr\Brace{\Stat_{i+1}|\Stat_{i}, \Policy(\Stat_i)} \cdot \tilde{W}^{(s)}\Paren{\Obsv(i+1)}
%         \nonumber\\
%         = \min_{\Policy(\Stat_i)}& \sum_{\Obsv^{(s)}(i+1)}
%             \Pr\Brace{\Obsv^{(s)}(i+1)|\Obsv^{(s)}(i), \Obsv^{(p)}(i), \Policy(\Stat_i)}
%             \nonumber\\
%             \times& \sum_{m\in\esSet} \sum_{j\in\jSpace}
%             \underbrace{
%                 \mathbb{E}_{\set{\Obsv^{(p)}(i+1)|\Policy(\Stat_i)}} \Bracket{\tilde{W}^{(s)}_{m,j}\Paren{\Obsv(i+1)}}
%                 }_{
%                 J\Paren{Q_{m,j}(i+1)}
%             },
% \end{align}
% where $\tilde{W}^{(s)}_{m,j}(\Obsv(i+1)) \define \mathbb{E}_{\Pi}[\sum_{l=0}^{\infty} \gamma^{l} q_{m,j}(i+l+1)]$. It implies that the state $\Obsv^{(p)}(i)$ is substitute with its expectation under policy $\Policy(\Stat_i)$ in this value function.

The expected value function $W^{(ES)}(\mathcal{Q}(t_{i+1}))$ under baseline $\vecG{\Pi}(t_{i})$ for ES nodes is little complex compared to Eqn. \ref{w_ap}.
The ES node transition is affected with both arrival process under dispatching policy and last queue state, and we reduce the states expression by taking average over the uploading process together with the transition function expression.
\begin{align}
    &W^{(ES)}_{\BaseLine(t_i)}\Paren{\mathcal{Q}(t_{i+1})} %\define \mathbb{E}_{\Pi}[\sum_{h=0}^{\infty} \gamma^{h} l_{m,j}(t_{i+h+1})]
    \nonumber\\
    =& \sum_{m,j}\sum_{h=0,\dots,\frac{\Xi}{T}} \gamma^{h} \mathbb{E}[ l_{m,j}(t_{i+h+1}) ]
    \nonumber\\
    &~~~~~~~~+ \gamma^{\frac{\Xi}{T}} \Paren{ \mat{I} - \gamma \hat{\mat{P}}_{m,j}(\tilde{\vecG{\beta}}) }^{-1} \vecG{\nu}(t_{i+\frac{\Xi}{T}+1}) \vec{g}',
\end{align}
where $\hat{\mat{P}}_{m,j}(\tilde{\vecG{\beta}}) \define \prod_{n=0,\dots,N-1} \mat{P}_{m,j}(\tilde{\vecG{\beta}})$;
$l_{m,j}(t_{i+h+1}) \sim \vecG{\nu}(t_{i+h+1})$, and $\vec{\nu}_{m,j}(t_{i+h+1})$ ($\forall h=0,\dots,]\frac{\Xi}{T}$) is obtained by calculation over equation (\ref{eqn_0})-(\ref{eqn_4});
the $i$-th element of vector $\vec{g}$ denotes the cost of server as $l_{m,j}(t_i)$;
$\tilde{\vecG{\beta}}$ is the arrival distribution under baseline policy $\Pi(t_{i})$ (on $m$-th ES with $j$-type job), where:
\begin{align}
    ~~~~\tilde{\vecG{\beta}} &\define [\tilde{\beta}^{(1)}_{m,j}, \tilde{\beta}^{(0)}_{m,j}]
    \\
    ~~~~\tilde{\beta}^{(1)}_{m,j} &\define \sum_{k\in\apSet} \lambda^{(k)}_{m,j} \times \sum_{\xi=0,\dots,\Xi-1} \Pr\{ \xi<U_{k,m}\leq(\xi+1) \}
        \nonumber\\
    ~~~~&= \sum_{k\in\apSet} \lambda^{(k)}_{m,j}
    \\
    ~~~~\tilde{\beta}^{(0)}_{m,j} &= 1 - \tilde{\beta}^{(1)}_{m,j}
\end{align}

%NOTE: non-sense
% Thus the approximated value function for $m$-th ES node is denoted as:
% \begin{align}
%     J\Paren{Q_{m,j}(i+1)} \define& \lim_{T\to\infty}
%         \mathbb{E}_{\vecG{\Pi}} \Bracket{\sum_{l=0}^{T} \gamma^{l} q_{m,j}(i+l+1)}
%         \nonumber\\
%         % =& \vec{u}'_i [\lim_{T\to\infty} \sum_{n=0}^{T} (\gamma \mat{P}_m)^{n}] \vec{g}_q
%         % \nonumber\\
%     =& \vec{\mu}'_{i+1} \Paren{ \mat{I}-\gamma\mat{P}_{m,j} }^{-1} \vec{g},
% \end{align}
% where $\vec{\mu}'_{i+1}$ denotes the transpose of $\vec{\mu}_{i+1}$, and $\vec{\mu}_{i+1} = [0\dots,0,1,0,\dots,0]$ with only $(i+1)$-th element as $1$; the $i$-th element of $\vec{g}$ denotes the cost of server as $q_{m,j}(i)$ for $i$-th stage.
%----------------------------------------------------------------------------------------%

\subsection{The Distributed Algorithm}
% [abandon; previous with uniform arrival]
% The approximate Bellman's equation under baseline policy is denoted as:
% \begin{align}
%     \min_{\Policy(\Stat_i)}& \sum_{\Obsv^{(s)}(i+1)} \Pr\{\Obsv^{(s)}(i+1)|\Obsv^{(s)}(i), \Obsv^{(p)}(i), \Policy(\Stat_i)\}
%     \nonumber\\
%     \times \Bracket{
%         & \sum_{m\in\esSet}\sum_{j\in\jSpace} J\Paren{Q_{m,j}(i+1)}
%         +
%         \nonumber\\
%         & \underbrace{\sum_{\Obsv^{(p)}(i+1)} \Pr\{\Obsv^{(p)}(i+1)|\Policy(\Stat_i)\}}_{\text{=1}}
%         \cdot \underbrace{\tilde{W}^{(p)} \Paren{\Obsv^{(p)}(i+1)}}_{\text{constant}}
%     },
% \end{align}
% and we have
% \begin{align}
%     \sum_{m\in\esSet}\sum_{j\in\jSpace} &\Pr\{Q_{m,j}(i+1)|Q_{m,j}(i), \Obsv^{(p)}(i), \Policy(\Stat_i)\} \mu'_{i+1}
%         \nonumber\\
%         &\Paren{ \mat{I}-\gamma\mat{P}_{m,j} }^{-1} \vec{g}
% \end{align}

The optimization problem at right-hand side of approximate Bellman's Equation is given as follows.
\begin{align}
    \min_{\BaseLine(t_i)} W^{(AP)}_{\BaseLine(t_i)}\Paren{\mathcal{R}(t_{i+1})} + W^{(ES)}_{\BaseLine(t_i)}\Paren{\mathcal{Q}(t_{i+1})}
    \label{eqn_opt}
\end{align}

Then we introduce the iterative policy update algorithm, which optimize the 
with improved baseline policy in each interval as performance guarantee.
\begin{itemize}
    \item Choose initial policy $\Policy(\Stat(t_0)) = \BaseLine(t_0)$ as start, which is the \emph{local greedy policy};
    \item In $[t_0, t_1]$, the AP node indexed with $1$ would receive the broadcast information after $D_1(t_1)$ and updates the global information; then it solves the optimization problem in \ref{eqn_opt} with only $\Pi_{1}(t_1)$ changes in $\Policy(\Stat(t_1)) = \BaseLine(t_1) = [\Pi_{1}(t_1), \Pi_{2}(t_0), \dots, \Pi_{K}(t_0)]$;
    \item At $t_1$, the AP node indexed with $1$ would broadcast its policy $\Pi_{1}(t_1)$ together with the state information;
    \item In $[t_1, t_2]$, the AP nodes indexed with $2$ would receive the broadcast information and previous global policy $\Policy(\Stat(t_1))$; then it repeats the procedure what the $1$-st AP did;
    \item Then generally, in $[t_{i}, t_{i+1}]$, the AP nodes indexed with $(k \mod K)+1$ would update its own policy following the above procedure.
\end{itemize}

% [\IF, \ENDIF], [\FOR, \TO, \ENDFOR], [\WHILE, \ENDWHILE], \STATE, \AND, \TRUE
% \begin{algorithm}[H]
%     \caption{Online Iterative Policy Improvement Algorithm}
%     \begin{algorithmic}[1]
%         \STATE $t = 0$
%         \FOR{$t = 1,2,\dots$}
%             \STATE Evaluate $\Omega_0$ in \textbf{P1} according to Eqn. \ref{sp1}
%             \FOR{$k \in \mathcal{K}$}
%                 \STATE fix policy $\vec{\Omega}^{(k)}(t) \forall k' < k$
%                 \STATE Evaluate $k$-th AP Local Policy $\tilde{\Omega}_k$ in \textbf{Pk} according to Eqn. \ref{sp2}
%             \ENDFOR
%         \ENDFOR
%     \end{algorithmic}
% \end{algorithm}

\begin{lemma}
    The above algorithm would have performance improvement in each interval with baseline policy guarantee, and at least achieve local optimal.
\end{lemma}
\begin{proof}
    The improved policy in last interval becomes the baseline policy in next interval.
    So, in each stage, it resembles the one-step Policy Improvement step in classical \emph{Policy Iteration} algorithm, and each step with different value function under baseline policy on the right-hand side.
    
    \fixit{The insight exists that: due to the uncontrollable delay on policy in each stage,} the chosen time-invariant baseline policy actually would; so the improved policy is the baseline of next stage, and thus forms the online policy iteration.
    discounted sum-up cost, the future states near the current state counts more and the baseline policy (which is time-invariant) would variant in each interval.
\end{proof}

\subsection{Reinforcement Learning}
Due to the fact that the statistical parameters of job arrival distribution $\lambda_{k,j}$, uploading time distribution $U_{k,m,j}$ and processing time distribution $l_{m,j}$ are unknown at the first time, we adopt reinforcement learning algorithm to learn the parameters.
%----------------------------------------------------------------------------------------%